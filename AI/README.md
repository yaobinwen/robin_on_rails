# Artificial Intelligence

- Many people use ChatGPT to generate code. I doubt if that's the best way to use it and how this kind of use can affect cognitive ability. It's like we stop using part of our mind and that part will get rusty. I think ChatGPT helps the best in learning new knowledge.
- [Use of nuclear and radiological weapons by terrorists?](https://international-review.icrc.org/sites/default/files/irrc_859_5.pdf)
- [Can Terrorists Build Nuclear Weapons?](https://www.nci.org/k-m/makeab.htm)

---

A few days ago, [Cognition](https://twitter.com/cognition_labs) introduced ["Devin, the first AI software engineer"](https://twitter.com/cognition_labs/status/1767548763134964000). Being a software engineer myself, this kind of news seems to be very bad news to me.

On the surface, it looks like there will be a job crisis to most, if not all, of the software engineers. But I believe there is an opportunity inside a crisis: The development of A.I. pushes us to rethink of the meaning of life, so this is probably the best opportunity to find the thing that really matters.

When asked, a lot of people may say the source of the meaning of their lives is to "make the world a better place." In other words, the world is not good enough yet and they believe their work can improve the situation. But what if A.I. is advanced enough and can do all the needed work, as long as they receive the command from the human beings? In that case, there would be no room for people to do anything. If they can't do anything, how do they find the meaning of their lives?

But AI may not be able to do "all" the work: AI may help do a lot of technical work, but when it comes to human relationships, we still need human beings themselves.

---

Do we need to understand the code that AI writes? It seems so. We probably need to understand what AI is doing. Otherwise, we can be easily deceived and manipulated by AI and just blindly trust them, giving them the opportunities to rule us. For example, if AI causes some damage and then tells us the damage is the inherent risk of the thing, how do we know AI is telling the truth or deceiving us?

Another example: Suppose AI #1 writes a piece of text that we human beings don't understand. So we ask AI #2 to translate it. AI #2 does it. But how do we know AI #2 translates it correctly, and does not secretly work with AI #1 to deceive us?

---

Many people have ideas of solving problems but don't know technologies; many people have technological knowledge but don't know what problems to solve. AI helps the first kind of people so they have more access to technological knowledge and thus can do more. Therefore, people need to focus on finding the right problems to solve.

---

How to guarantee that human beings are always the master of AI?

---

When AI can replace the human beings on a massive scale, i.e., most of the people can't make money because they can't find a job now, how to maintain their living?

Likewise, when AI can make more fortune than human beings do, how to distribute the fortune?

---

Can AI make more AIs?
