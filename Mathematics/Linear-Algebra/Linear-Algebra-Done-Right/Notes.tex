% Regarding `oneside` (https://stackoverflow.com/a/8371473/630364):
%
% `oneside` removes the blank pages between chapters.
% "Note that this method make the margins of all the pages the same. In
% `twoside`, the margins are different for the odd and the even pages".
\documentclass[12pt, letterpaper, oneside]{book}
\usepackage{amsmath}
\usepackage[letterpaper, textwidth=7.5in, textheight=8in]{geometry}
\usepackage{hyperref}
\usepackage{csquotes}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
}

\title{Notes on \textit{Linear Algebra Done Right}}
\author{Yaobin Wen}
\date{January 2023}

\begin{document}

\maketitle
\tableofcontents

\chapter*{Overview}
\addcontentsline{toc}{chapter}{Overview}

This document contains my study notes of the textbook \textit{Linear Algebra
Done Right}. I use it for a few purposes:

\begin{enumerate}
  \item As a reference to quickly refresh my memory on the subjects.
  \item Keep the notes to help me understand the text that is not obvious for
    me to comprehend.
\end{enumerate}

% =============================================================================
%
% Chapter: Review of Real Numbers
%
% =============================================================================

\chapter*{Review of Real Numbers}
\addcontentsline{toc}{chapter}{Review of Real Numbers}

(TODO)

% =============================================================================
%
% Chapter 1: Vector Spaces
%
% =============================================================================

\chapter{Vector Spaces}

% ------------------------------
\section{Overview}
% ------------------------------

This chapter starts with the definition of \emph{complex numbers} because
complex numbers are a superset of real numbers. Once we properly define complex
numbers, we can focus on them because the properties will also hold for real
numbers. "In linear algebra," as the textbook says, "better theorems and more
insight emerge if complex numbers are investigated along with real numbers."

A complex number is an element of a \emph{list}, and a list is used to denote a
\emph{vector}, and vectors make up \emph{vector spaces}. Therefore, after
complex numbers are discussed, we will move onto defining lists, vectors, and
vector spaces.

% ------------------------------
\section{Review Questions}
% ------------------------------

(See the end of this chapter for the review question answers.)

\begin{enumerate}
  \item What is a vector space?
  \item What is the relationship between $\mathbf{F^n}$ and the vector space
    $V$?
  \item In a vector space $V$, for $u, v \in V$, what is the relationship
    between $u \cdot v$ and $v \cdot u$?
  \item In a vector space $V$, for $u, v, w \in V$, what is the relationship
  between $(u \cdot v) \cdot w$ and $u \cdot (v \cdot w)$?
  \item When talking about the vector space $V$ as the textbook defines it, why
    is it important to specify that it depends on the set $F$? What role does
    $\mathbf{F}$ play in the definition of $V$?
  \item What is $\mathbf{F}^S$?
  \item In the definition of $\mathbf{F}^S$, what does $S$ stand for? What is
    an element in $\mathbf{F}^S$?
  \item Does the definition of $\mathbf{F}^S$ imply that the function
    $0: S \rightarrow \mathbf{F}$ is an element in $\mathbf{F}^S$? Why or why
    not?
  \item In section ``1.29 The number 0 times a vector'', the textbook proves
    the property $0v = 0$ for every $v \in V$ without using the scalar
    multiplication in $\mathbf{F}^n$ in section 1.17. Why not use that? Should
    it have been used, the proof would have been much more simple, wouldn't it?
  \item What is a subspace?
  \item What are the conditions for the subset $U$ to satisfy in order to make
    it a subspace of $V$?
  \item Among all the conditions that $U$ must satisfy, what about the
    multiplicative identity $1$?
  \item What is the sum of subsets?
  \item What is a direct sum?
  \item How is direct sum different from sum of subsets?
\end{enumerate}

% ------------------------------
\section{1.1 Definition: Complex numbers}
% ------------------------------

We {define} a \textbf{\emph{complex number}} as follows (see
``1.1 Definition''):

\begin{itemize}
  \item A complex number is an ordered pair $(a, b)$, where $a, b \in
    \mathbf{R}$, but we will write this as $a + bi$.
    \begin{itemize}
      \item When $a, b \in \mathbf{R}$ and $b = 0$, we have all the real
        numbers $\mathbf{R}$. Thus $\mathbf{R} \subset \mathbf{C}$.
    \end{itemize}
  \item The set of all complex numbers is denoted by $\mathbf{C}$:
    \[ \mathbf{C} = \{a + bi: a, b \in \mathbf{R}\} \]
  \item \textbf{Addition}: Let $\alpha = (a + bi)$, $\beta = (c + di)$, where
    $a, b, c, d \in \mathbf{R}$, we have:
    \[ \alpha + \beta = (a + bi) + (c + di) = (a + c) + (b + d)i \]
    \begin{itemize}
      \item This also tells us that if $\alpha, \beta \in \mathbf{C}$, then
        $\alpha + \beta \in \mathbf{C}$
    \end{itemize}
  \item \textbf{Multiplication}: Let $\alpha = (a + bi)$, $\beta = (c + di)$,
    where $a, b, c, d \in \mathbf{R}$, we have:
    \[ \alpha\beta = (a + bi)(c + di) = (ac - bd) + (ad + bc)i \]
    \begin{itemize}
      \item This also tells us that if $\alpha, \beta \in \mathbf{C}$, then
        $\alpha\beta \in \mathbf{C}$
    \end{itemize}
\end{itemize}

% ------------------------------
\section{1.3 Properties of complex arithmetic}
% ------------------------------

The following properties can be proved according to the definition of complex
numbers and the addition and multiplication:

\begin{itemize}
  \item \textbf{commutativity}:
    \[
      \alpha + \beta = \beta + \alpha \quad and \quad \alpha\beta\ =
      \beta\alpha \quad for \; all \; \alpha, \beta \in \mathbf{C}
    \]
  \item \textbf{associativity}:
    \[
      (\alpha + \beta) + \lambda = \alpha + (\beta + \lambda) \quad and \quad
      (\alpha\beta)\lambda = \alpha(\beta\lambda) \quad for \; all \; \alpha,
      \beta, \lambda \in \mathbf{C}
    \]
  \item \textbf{identities}:
    \[
      \lambda + 0 = \lambda \ and \ \lambda1 = \lambda \ for \ all \ \lambda
      \in \mathbf{C}
    \]
    \begin{itemize}
      \item Note that here $0, 1 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{additive inverse}:
    \[
      For \ every \ \alpha \in \mathbf{C}, there \ exists \ a \ unique \ \beta
      \in \mathbf{C} \ such \ that \ \alpha + \beta = 0
    \]
    \begin{itemize}
      \item Note that here $0 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{multiplicative inverse}:
    \[
      For \ every \ \alpha \in \mathbf{C} \ with \ \alpha \neq 0, there \
      exists \ a \ unique \ \beta \in \mathbf{C} \newline such \ that \
      \alpha\beta = 1
    \]
    \begin{itemize}
      \item Note that here $1 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{distributive property}:
    \[
      \lambda(\alpha + \beta) = \lambda\alpha + \lambda\beta \ for \ all \
      \lambda, \alpha, \beta \in \mathbf{C}
    \]
\end{itemize}

NOTE(ywen): Complex numbers are defined based upon real numbers. In order to
prove the properties of complex arithmetic, we need to use the properties of
real arithmetic (i.e., communtativity, associativity, etc. for real numbers).
Example 1.4 shows this method.

% ------------------------------
\section{1.5 Definition: $-\alpha$, subtraction, $1/\alpha$, division}
% ------------------------------

Let $\alpha, \beta \in \mathbf{C}$:

\begin{itemize}
  \item ``$-\alpha$'' is the additive inverse of $\alpha$ such that
    \[ \alpha + (-\alpha) = 0 \]
    \begin{itemize}
      \item Note that here $0 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{\textit{Subtraction}} on $\mathbf{C}$ is defined by
    \[ \beta - \alpha = \beta + (-\alpha) \]
  \item For $\alpha \neq 0$, define $1/\alpha$ as the \textbf{multiplicative
    inverse} of $\alpha$ such that
    \[ \alpha(1/\alpha) = 1 \]
    \begin{itemize}
      \item Note that here $1 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{\textit{Division}} on $\mathbf{C}$ is defined by
    \[ \beta/\alpha = \beta(1/\alpha) \]
    \begin{itemize}
      \item Note that here $1 \in \mathbf{R}$
    \end{itemize}
\end{itemize}

% ------------------------------
\section{1.6 Notation F}
% ------------------------------

$\mathbf{F}$ stands for either $\mathbf{R}$ or $\mathbf{C}$.

But because $\mathbf{C}$ is a superset of $\mathbf{R}$, when we think of
$\mathbf{F}$, we can basically think of it as $\mathbf{C}$.

% ------------------------------
\section{1.6.1 Definition: $\alpha^m$}
% ------------------------------

For $\alpha \in \mathbf{F}$ and $m \in \mathbf{N}$:

\[ \alpha^m = \underbrace{\alpha\cdot\cdot\cdot\alpha}_{m \ times} \]

Thus for all $\alpha, \beta \in \mathbf{F}$, $m, n \in \mathbf{N}$:

\begin{itemize}
  \item $(\alpha^m)^n = \alpha^{mn}$
  \item $(\alpha\beta)^m = \alpha^m\beta^m$
\end{itemize}

% ------------------------------
\section{1.8 Definition: \textit{list}, \textit{length}}
% ------------------------------

For $n \in \mathbf{N}$, a \textbf{\textit{list}} of \textbf{\textit{length n}}
is an \textbf{ordered} collection of \textit{n} \textbf{elements}. It's denoted
as follows: \[ (x_1, x_2, ..., x_n) \]

Note that:
\begin{itemize}
  \item A list has a \textbf{finite} length.
  \item An element can be anything: a number, another list, a text string, etc.
  \item A list of length 0 is denoted as ``$()$''.
  \item A list of length n is also called an \textit{n-\textbf{tuple}}.
\end{itemize}

Two lists $(x_1, x_2, ..., x_n)$ and $(y_1, y_2, ..., y_m)$ are equal if and
only if:
\begin{itemize}
  \item $n = m$
  \item $x_i = y_i$ for $i = 1, 2, ..., n$
\end{itemize}

% ------------------------------
\section{Lists and Sets}
% ------------------------------

Table compares lists and sets:
\begin{table}[ht!]
\centering
\begin{tabular}{||c c c ||} 
 \hline
   & Lists & Sets \\ [0.5ex] 
 \hline
 \hline
 Length & Finite & Finite or infinite \\ 
 Order & Matters & Doesn't matter \\
 Repetition & Allows & Doesn't allow \\ [1ex]
 \hline
\end{tabular}
\caption{Compare lists and sets}
\label{table:lists_sets_comp}
\end{table}

% ------------------------------
\section{1.10 Definition: $\mathbf{F^n}$ and $0$}
% ------------------------------

We use $\mathbf{F}$ to denote either $\mathbf{R}$ or $\mathbf{C}$.

We define $\mathbf{F^n}$ as the set of all lists of length $n$ of elements of
$\mathbf{F}$:

\[
  \mathbf{F^n} = \{(x_1, ..., x_n): x_j \in \mathbf{F} \ for \ j = 1, ..., n\}
\]

We say $x_j$ is the $j^{th}$ \textbf{\textit{coordinate}} of $(x_1, ..., x_n)$.

Let $\mathbf{0}$ denote the list of length $n$ whose coordinates are all $0$:

\[
  \mathbf{0} = (0, 0, ..., 0)
\]

% ------------------------------
\section{1.11 Note: $\mathbf{C^1}$ can be thought of as a plane}
% ------------------------------

On page 6, below Example 1.11, the textbook says: ``Similarly, $\mathbf{C}^1$
can be thought of as a plane.''

It is so because $\mathbf{C}^1$ is the set of complex numbers, each of which
consists of two parts: the real part and the imaginary part. Both parts are real
numbers and they can form a 2D plane, hence ``as a plane''.

% ------------------------------
\section{1.12 Vectors}
% ------------------------------

For $x \in \mathbf{F}^n$, we can view it as a point in the space $\mathbf{F}^n$,
but we can also view it as an ``arrow'' in the same space. When we view it as
an ``arrow'', we refer to it as a \textbf{\textit{vector}}.

% ------------------------------
\section{1.13 Definition: \textit{addition} in $\mathbf{F}^n$}
% ------------------------------

Given

\[
  \mathbf{F^n} = \{(x_1, ..., x_n): x_j \in \mathbf{F} \ for \ j = 1, ..., n\}
\]

and two lists $x, y \in \mathbf{F}^n$, and

\[
  x = (x_1, x_2, ..., x_n)
\]
\[
  y = (y_1, y_2, ..., y_n)
\]

we have

\begin{equation*}
\begin{split}
  x + y &= (x_1, x_2, ..., x_n) + (y_1, y_2, ..., y_n) \\
        &= (x_1 + y_1, x_2 + y_2, ..., x_n + y_n)
\end{split}
\end{equation*}

The addition has the property of \textbf{\textit{commutativity}}: If $x, y \in
\mathbf{F}^n$, then $x + y = y + x$.

The \textbf{\textit{additive inverse}} of $x$, denoted as $-x$, is the vector
$-x \in \mathbf{F}^n$ such that

\[
  x + (-x) = 0
\]

In other words, if $x = (x_1, ..., x_n)$, then $-x = (-x_1, ..., -x_n)$.

% ------------------------------
\section{1.14 Definition:\
  \textbf{\textit{scalar multiplication}} in $\mathbf{F}^n$}
% ------------------------------

For $\lambda \in \mathbf{F}$ and $(x_1, ..., x_n) \in \mathbf{F}^n$, we define

\[
  \lambda(x_1, ..., x_n) = (\lambda x_1, ..., \lambda x_n)
\]

% ------------------------------
\section{1.18 Definition: \it{addition}, \it{scalar multiplication}}
% ------------------------------

\begin{itemize}
  \item An \textbf{\textit{addition}} on a set $V$ is a function that assigns
    an element $u+v \in V$ to each pair of element $u, v \in V$.
  \item A \textbf{\textit{scalar multiplication}} on a set $V$ is a function
    that assigns an element $\lambda v \in V$ to each $\lambda \in \mathbf{F}$
    and each $v \in V$.
\end{itemize}

% ------------------------------
\section{1.19 Definition: \textbf{\textit{vector space}}}
% ------------------------------

A \textbf{\textit{vector space}} is a set $V$ along with an \textit{addition}
on $V$ and a \textit{scalar multiplication} on $V$ such that the following
properties hold:

\textbf{commutativity}: $u + v = v + u$ for all $u, v \in V$. \textbf{Note}
that it doesn't define the relationship between $uv$ and $uv$.

\textbf{associativity}: For all $u, v, w \in V$ and all $a, b \in \mathbf{F}$:
\begin{itemize}
  \item $(u + v) + w = u + (v + w)$
  \item $(ab)v = a(bv)$
\end{itemize}

\textbf{Note} that associativity doesn't define the relationship between
$(uv)w$ and $u(vw)$.

\textbf{additive identity}: For all $v \in V$, there exists an element $0 \in V$
such that $v + 0 = v$. \textbf{Note} this doesn't require uniqueness of $0$.

\textbf{additive inverse}: For all $v \in V$, there exists $w \in V$ such that
$v + w = 0$. \textbf{Note} this doesn't require uniqueness of $w$.

\textbf{multiplicative identity}: $1v = v$ for all $v \in V$. \textbf{Note}
  that $1 \in \mathbf{F}$.

\textbf{distributive properties}: For all $a, b \in \mathbf{F}$ and all $u, v
\in V$:
\begin{itemize}
  \item $a(u + v) = au + av$
  \item $(a + b)v = av + bv$
\end{itemize}

% ------------------------------
\section{What do communtativity and associativity mean for additions on vector
  spaces?}
% ------------------------------

The \textbf{additions} a vector space $V$ has the properties of communtativity
and associativity. What that means essentially is, for $u, v, w \in V$, one can
calculate the the \textbf{addition} of $u, v, w$ in \textbf{any order} with
\textbf{any association}. In other words, all the following calculations are
equal:

\begin{itemize}
  \item $u + v + w$
  \item $u + w + v$
  \item $u + (v + w)$
  \item $v + u + w$
  \item $v + (u + w)$
  \item $w + v + u$
  \item etc.
\end{itemize}

It's easy to prove that. But to prove it, we need to explicitly mention a rule:
\[ u + v + w = (u + v) + w \]

Namely, $u + v$ is just a simpler form of writing $(u + v)$ because there is no
ambiguity in the order of calculation. With that said, we have:

\begin{equation*}
  \begin{split}
    u + v + w &= (u + v) + w \\
              &= u + (v + w) \\
              &= u + (w + v) \\
              &= (u + w) + v = u + w + v \\
              &= (w + u) + v \\
              &= w + (u + v) \\
              &= w + (v + u) \\
              &= (w + v) + u \\
              &= w + v + u \\
              &= \ldots 
  \end{split}
\end{equation*}

The trick is to use the \textbf{communtativity} and \textbf{associativity}
repeatedly.

% ------------------------------
\section{Properties 1.25 - 1.31}
% ------------------------------

\begin{itemize}
  \item 1.25: Unique additive identity: A vector space has a unique additive
    identity.
  \item 1.26 Unique additive inverse: Every element in a vector space has a
    unique additive inverse.
  \item 1.27 Notation $-v$, $w - v$: Let $v, w \in V$, then:
    \begin{itemize}
      \item[$\bullet$] $-v$ denotes the additive inverse of $v$.
      \item[$\bullet$] $w - v$ is defined to be $w + (-v)$.
    \end{itemize}
  \item 1.28 Notation $V$: For the rest of the book, $V$ denotes a vector space
    over $\mathbf{F}$.
  \item 1.29 The number 0 times a vector: $0v = 0$ for every $v \in V$.
  \item 1.30 A number times the vector 0: $a0 = 0$ for every $a \in \mathbf{F}$.
  \item 1.31 The number $-1$ times a vector: $(-1)v = -v$ for every $v \in V$.
\end{itemize}

% ------------------------------
\section{1.20 Definition: \textbf{\textit{vector}}, \textbf{\textit{point}}}
% ------------------------------

Elements of a vector space are called \textbf{\textit{vector}} or
\textbf{\textit{point}}.

% ------------------------------
\section{My understanding of vector space}
% ------------------------------

\begin{itemize}
  \item Essentially, ``addition'' of two vectors is to connect the tail of the
    first vector with the head of the second vector.
  \item Essentially, ``scalar multiplication'' of a vector is to shrink or
    stretch the vector.
  \item The definition of \textit{vector space} is saying: For an arbitrary set
    $V$, we can define arbitrary operations called \textit{addition} and
    \textit{scalar multiplication} on the elements (assuming ``scalar'' makes
    sense in the context). However, not all the addition and scalar
    multiplication can make $V$ meet the properties that are required by vector
    space definition. In other occasions, $V$ may not include all the elements
    that make the addition and scalar multiplication meet the properties that
    are required by the vector space definition. Only those sets $V$ and the
    corresponding addition and scalar multiplication that meet those properties
    are called \textit{vector spaces}.
\end{itemize}

% ------------------------------
\section{1.23 Notation $F^S$}
% ------------------------------

\begin{itemize}
  \item If $S$ is a set, then $\mathbf{F}^S$ denotes the set of
    \textbf{functions} from $S$ to $\mathbf{F}$.
  \item For $f,g \in \mathbf{F}^S$, the \textbf{sum} $f + g \in \mathbf{F}^S$
    is the function defined by
      \[ (f + g)(x) = f(x) + g(x) \]
    for all $x \in S$.
  \item For $\lambda \in \mathbf{F}$ and $f \in \mathbf{F}^S$, the
    \textbf{product} $\lambda f \in \mathbf{F}^S$ is the function defined by
      \[ (\lambda f)(x) = \lambda f(x) \]
    for all $x \in S$.
\end{itemize}

% ------------------------------
\section{1.24 Example $\mathbf{F}^S$ is a vector space}
% ------------------------------

\begin{itemize}
  \item If $S$ is a \textbf{nonempty set}, then $\mathbf{F}^S$ (with the
   operations of addition and scalar multiplication as defined in section 1.23)
   is a vector space over $\mathbf{F}$.
  \item The additive identity of $\mathbf{F}^S$ is the function $0:
    S \rightarrow \mathbf{F}$ defined by
      \[ 0(x) = 0 \]
    for all $x \in S$.
  \item For $f \in \mathbf{F}^S$, the additive inverse of $f$ is the function
    $-f: S \rightarrow \mathbf{F}$ defined by
      \[ (-f)(x) = -f(x) \]
    for all $x \in S$.
\end{itemize}

% ------------------------------
\section{My understanding of $\mathbf{F}^S$} \label{F^S understanding}
% ------------------------------

\begin{enumerate}
  \item Note that the elements in $\mathbf{F}^S$ are \textbf{functions}.
    According to the definition of $\mathbf{F}^S$, the only condition these
    functions must satisfy is they map from $S$ to $\mathbf{F}$.
  \item It's also important to note that it doesn't matter how the mapping from
    $S$ to $\mathbf{F}$ is defined. For example, it doesn't say that every
    element in $S$ must be mapped to a different element on $\mathbf{F}$. In
    fact, it's possible that all the element in $S$ are just mapped to one
    element in $\mathbf{F}$. Again, the point is the mapping itself does not
    matter, as long as the mapping is from $S$ to $\mathbf{F}$.
  \item \label{mapping rules} Because the mapping rule doesn't matter, that
    means \textbf{all possible mappings} (i.e., all possible functions) from
    $S$ to $\mathbf{F}$ are included in $\mathbf{F}^S$. For example:
    \begin{enumerate}
      \item Function $f_1: x \rightarrow x$ for all $x \in S$
      \item Function $f_2: x \rightarrow x + 1.339$ for all $x \in S$
      \item Function $f_3: x \rightarrow 2x^3 + 9\pi$ for all $x \in S$
      \item Function $f_4: x \rightarrow 1$ for all $x \in S$
      \item Function $f_5:$
        \[
          x \rightarrow \begin{cases}
            1, & if x = 13.5 \\
            0,  & if x \neq 13.5 \\
          \end{cases}
        \]
        for all $x \in S$.
      \item Function $f_6: x \rightarrow 0$ for all $x \in S$
      \item etc.
    \end{enumerate}
    Notice that $f_6$ is the \textbf{additive identity} in $\mathbf{F}$.
  \item The type of elements of $S$ doesn't matter either, according to the
    definition. For example, the elements in $S$ don't have to be numbers. They
    can be shapes like $\{ \clubsuit, \diamondsuit, \heartsuit, \spadesuit \}$.
  \item The textbook says $\mathbf{F}^n$ and $\mathbf{F}^{\infty}$ can be
    viewed as special cases of $\mathbf{F}^S$. This is correct but the textbook
    doesn't explain everything about them:
    \begin{itemize}
      \item In these two cases, $S$ is the set $\{ 1, 2, \ldots, n \}$ and
      $\{ 1, 2, \ldots, \}$, respectively.
      \item An element in $\mathbf{F}^n$, i.e., $\{ x_1, x_2, \ldots, x_n \}$,
        is written as a list but represents a \textbf{function}. Yes, we
        usually use the symbol $f$ to denote a function, but we don't have to.
        If we want to, we can choose any arbitrary symbols to denote functions,
        including using a list. In the case of $\mathbf{F}^n$, the list
        $\{ x_1, x_2, \ldots, x_n \}$ and $\{ y_1, y_2, \ldots, y_n \}$ can be
        used to denote two different functions, just in the same way that $f$
        and $g$ can denote different functions.
    \end{itemize}
\end{enumerate}

% ------------------------------
\section{1.32 Definition \textit{subspace}}
% ------------------------------

A subset $U$ of $V$ is called a \textbf{\textit{subspace}} of $V$ if $U$ is
also a vector space (using the same addition and scalar multiplication as on
$V$).

% ------------------------------
\section{1.34 Conditions for a subspace}
% ------------------------------

A subset $U$ of $V$ is a subspace of $V$ \textbf{if and only if} $U$ satisfies
the following three conditions:

\begin{itemize}
  \item \textbf{additive identity}: $0 \in U$
  \item \textbf{closed under addition}: $u, w \in U$ implies $u + w \in U$.
  \item \textbf{closed under scalar multiplication}: $\lambda \in \mathbf{F}$
    and $u \in U$ implies $\lambda u \in U$.
\end{itemize}

% ------------------------------
\section{1.36 Definition \textbf{\textit{sum of subsets}}}
% ------------------------------

\textbf{NOTE}: This definition is about \textbf{\textit{subsets}}, not
subspaces.

Suppose $U_1, \ldots, U_m$ are \textbf{subsets} of $V$. The \textbf{sum} of
$U_1, \ldots, U_m$, denoted $U_1 + \ldots + U_m$, is the set of \textbf{all
possible} sums of elements of $U_1, \ldots, U_m$. More precisely,

\[
U_1 + \ldots + U_m = \{ u_1 + \ldots + u_m: u_1 \in U_1, \ldots, u_m \in U_m \}
\]

% ------------------------------
\section{1.39 Sum of subspaces is the smallest containing subspace}
% ------------------------------

Suppose $U_1, \ldots, U_m$ are subspaces of $V$. Then $U_1 + \ldots + U_m$ is
the \textit{smallest subspace} of $V$ containing $U_1, \ldots, U_m$.

Proof: The proof consists of a few parts.

Part 1: Prove that $U_1 + \ldots + U_m$ is a subspace of $V$.

Firstly, let's prove that the additive identity $0 \in U_1 + \ldots + U_m$.
Because $U_1, \ldots, U_m$ are subspaces of $V$, according to the conditions of
subspaces, we know that the additive identity $0$ of $V$ must be in all of
$U_1, \ldots, U_m$. In other words:

\begin{align*}
      0 & \in U_1 \\
      0 & \in U_2 \\
        & \ldots \\
  + \ 0 & \in U_m \\
  = \ 0 & \in U_1 + U_2 + \ldots + U_m
\end{align*}

So we can see that additive identity $0 \in U_1 + \ldots + U_m$.

Secondly, let's prove that addition is closed under $U_1 + \ldots + U_m$. Let's
use $W$ to denote $U_1 + \ldots + U_m$, and the two elements $w_1, w_2 \in W$
can be denoted as follows:

\[
  w_1 = u_{11} + \ldots + u_{m1}, u_{11} \in U_1, \ldots, u_{m1} \in U_m
\]

and

\[
  w_2 = u_{12} + \ldots + u_{m2}, u_{12} \in U_1, \ldots, u_{m2} \in U_m
\]

So $w_1 + w_2 = (u_{11} + \ldots + u_{m1}) + (u_{12} + \ldots + u_{m2})$.

Because $u_{11}, \ldots, u_{m1}, u_{12}, \ldots, u_{m2} \in V$, the addition of
these elements can be performed in any order with any association. Therefore:

\begin{align*}
  w_1 + w_2 & = (u_{11} + \ldots + u_{m1}) + (u_{12} + \ldots + u_{m2}) \\
            & = u_{11} + \ldots + u_{m1} + u_{12} + \ldots + u_{m2} \\
            & = (u_{11} + u_{12}) + \ldots + (u_{m1} + u_{m2})
\end{align*}

Because $u_{11}, u_{12} \in U_1$ and $U_1$ is a subspace, that means $u_{11} +
u_{12} \in U_1$. The same can be applied to $u_{i1}, u_{i2}$ so we know
$u_{i1} + u_{i2} \in U_i$ where $1 \leq i \leq m$.

Therefore, we can tell that $w_1 + w_2$ is the sum of $u_i \in U_i$ where
$1 \leq i \leq m$, so $w_1 + w_2 \in U_1 + \ldots + U_m$. So addition is closed
under $U_1 + \ldots + U_m$.

Thirdly, let's prove that scalar multiplication is closed under $U_1 + \ldots +
U_m$ using the denotation above. For $\lambda \in \mathbf{F}$, we have

\[
  \lambda w_1 = \lambda (u_{11} + \ldots + u_{m1})
\]

Because $u_{11}, \ldots, u_{m1} \in V$, the distributive property holds, so

\begin{align*}
  \lambda w_1 & = \lambda (u_{11} + \ldots + u_{m1}) \\
              & = \lambda u_{11} + \ldots + \lambda u_{m1}
\end{align*}

Because $u_{11} \in U_1$ and $U_1$ is a subspace, that means $\lambda u_{11}
\in U_1$ because of the closure on scalar multiplication. The same can be
applied to $\lambda u_{i1}$ so we know $\lambda u_{i1} \in U_i$ where $1 \leq i
\leq m$.

Therefore, we can tell that $\lambda w_1$ is the sum of $\lambda u_{i1}$ where
$1 \leq i \leq m$, so $\lambda w_1 \in U_1 + \ldots + U_m$. So scalar
multiplication is closed under $U_1 + \ldots + U_m$.

Part 2: Prove that $U_1 + \ldots + U_m$ is the \textit{smallest} subspace of
$V$ containing $U_1, \ldots, U_m$.

Firstly, the textbook proves that $U_1, \ldots, U_m$ are all contained in
$U_1 + \ldots + U_m$. The textbook says:

\begin{displayquote}
  to see this, consider sums $u_1 + \ldots + u_m$ where all except one of the
  $u$'s are 0.
\end{displayquote}

What the textbook means is: Consider the sums $u_1 + u_2 + \ldots + u_m$ where
$u_2, \ldots, u_m$ are all the additive identity $0$. As a result, $u_1$ plus a
$m-1$ number of $0$ still produce $u_1$. Because $u_1$ means any arbitrary
element in $U_1$, that means $u_1 + u_2 + \ldots + u_m$ is essentially any
arbitrary element in $U_1$, so all the possible selection of $u_1$ will result
in the entire set $U_1$, which means the set $U_1$ is contained in $U_1 +
\ldots + U_m$. Then we can apply the same process to $u_i$ where $2 \leq i \leq
m$ to prove that the set $U_i$ is contained in $U_1 + \ldots + U_m$ where $2
\leq i \leq m$. Therefore, we can prove that $U_1, \ldots, U_m$ are all
contained in $U_1 + \ldots + U_m$.

Secondly, the textbook proves that every subspace of $V$ containing $U_1,
\ldots, U_m$ also contains $U_1 + \ldots + U_m$. The textbook says:

\begin{displayquote}
  ...because subspaces must contain all finite sums of their elements
\end{displayquote}

What the textbook means is: Now we are considering a subspace of $V$ that
must contain $U_1, \ldots, U_m$. Let's denote this subspace with $U$. Because
$U$ is a subspace, it must be closed under addition, which means $\forall u, w
\in U$, we have $u + w \in U$. Because $U_1$ and $U_2$ are contained in $U$,
$u_1 \in U_1 \Rightarrow u_1 \in U$, and $u_2 \in U_1 \Rightarrow u_2 \in U$,
therefore, $u_1 + u_2 \in U$. Let's denote $u_1 + u_2$ as $w_{u_1+u_2}$, so we
have $w_{u_1+u_2} \in U$. Now, because $U_3$ is contained in $U$, $\forall u_3
\in U_3 \Rightarrow u_3 \in U$. Therefore, $w_{u_1+u_2} + u_3 \in U \Rightarrow
(u_1 + u_2) + u_3 \in U \Rightarrow u_1 + u_2 + u_3 \in U$. We can apply the
same process for $u_i$ where $4 \leq i \leq m$. Once we get to $m$, we have
$u_1 + u_2 + \ldots + u_m \in U$. This means $\forall u_1 + u_2 + \ldots + u_m
\in U$, so we know $U_1 + \ldots + U_m \subseteq U$. In other words, every
subspace of $V$ containing $U_1, \ldots, U_m$ also contains $U_1 + \ldots +
U_m$. In other words, if a subset of $V$ does not contain $U_1 + \ldots + U_m$,
it can't be a subspace of $V$ containing $U_1, \ldots, U_m$. Therefore, any
subset $W \subset U_1 + \ldots + U_m$ is not such a subspace. The smallest
subset of $V$ that contains $U_1 + \ldots + U_m$ is $U_1 + \ldots + U_m$
itself. Because $U_1 + \ldots + U_m$ is also a subspace of $V$ containing $U_1,
\ldots, U_m$, we know $U_1 + \ldots + U_m$ is the \textit{smallest subspace} of
$V$ containing $U_1, \ldots, U_m$.

% ------------------------------
\section{1.40 Definition \textbf{\textit{direct sum}}}
% ------------------------------

Suppose $U_1, \ldots, U_m$ are subspaces of $V$:
\begin{itemize}
  \item The sum $U_1 + \ldots + U_m$ is called a \textbf{\textit{direct sum}}
    if each element of $U_1 + \ldots + U_m$ can be written in \textbf{only one
    way} as a sum $u_1 + \ldots + u_m$, where each $u_j$ is in $U_j$.
  \item If $U_1 + \ldots + U_m$ is a direct sum, then $U_1 \oplus \ldots \oplus
    U_m$ denotes $U_1 + \ldots + U_m$, with the $\oplus$ notation serving as an
    indication that this is a direct sum.
\end{itemize}

Example 1.41: Suppose $U$ is the subpsace of $\mathbf{F}^3$ of those vectors whose
last coordinate equals $0$, and $W$ is the subspace of $\mathbf{F}^3$ of those
vectors whose first two coordinates equal $0$:
\[
  U = {(x, y, 0) \in \mathbf{F}^3: x, y \in \mathbf{F}}
\]
and
\[
  W = {(0, 0, z) \in \mathbf{F}^3: x, y \in \mathbf{F}}
\]
Then $\mathbf{F}^3 = U \oplus W$.

Proof: Suppose any element $(x, y, z)$ in $U+W$ can be expressed in two ways:
\begin{enumerate}
  \item $(x, y, z) = (x_1, y_1, 0) + (0, 0, z_1) = (x_1 + 0, y_1 + 0, 0 + z_1)$
  \item $(x, y, z) = (x_2, y_2, 0) + (0, 0, z_2) = (x_2 + 0, y_2 + 0, 0 + z_2)$
\end{enumerate}

So we have:
\begin{enumerate}
  \item $x_1 + 0 = x = x_2 + 0 \Rightarrow x_1 = x_2$
  \item $y_1 + 0 = y = y_2 + 0 \Rightarrow y_1 = y_2$
  \item $z_1 + 0 = z = z_2 + 0 \Rightarrow z_1 = z_2$
\end{enumerate}

Therefore, $(x, y, z)$ has only one way to be expressed, so $U+W$ is a direct
sum and can be denoted as $U \oplus W$.

% ------------------------------
\section{Example 1.43: A question}
% ------------------------------

See the textbook for the example description and proof.

The point of this example is to show that $U_1 + U_2 + U_3$ is not a direct
sum. According to the definition of direct sum, a direct sum is defined on the
subspaces within a certain vector space $V$. However, the defintion does not
say that the direct sum of the subspaces must be equal to the entire vector
space $V$.

Therefore, I was confused by the proof of this example that's provided by the
textbook, because the paragraph after ``However'' seems to change the question
from proving that $U_1 + U_2 + U_3$ is not a direct sum to that
``$\mathbf{F}^3$ does not equal the direct sum of $U_1, U_2, U_3$.''

If we want to prove that $U_1 + U_2 + U_3$ is not a direct sum, we only need to
find one counter-example, which the proof already suggests: For any element
$(x, y, z)$ in $U_1 + U_2 + U_3$, we can find at least two expressions:

\begin{enumerate}
  \item $(x, y, z) = (x, y, 0) + (0, 0, z) + (0, 0, 0)$, and
  \item $(x, y, z) = (x, y + 1, 0) + (0, 0, z + 1) + (0, -1, -1)$
\end{enumerate}

This proves that there is no unique way to write the sum, so $U_1 + U_2 + U_3$
is not a direct sum.

% ------------------------------
\section{1.44 Condition for a direct sum}
% ------------------------------

Suppose $U_1, \ldots, U_m$ are subspaces of $V$. Then $U_1 + \ldots + U_m$ is a
direct sum \textbf{if and only if} the only way to write $0$ as a sum $u_1 +
\ldots + u_m$, where each $u_j$ is in $U_j$, is by taking each $u_j$ equal to 0.

A note about the proof: While studying the proof, when I saw the following
expressions:

\begin{itemize}
  \item $v = u_1 + \ldots + u_m$, and
  \item $v = v_1 + \ldots + v_m$
\end{itemize}

I started to think about the similar expressions in integers like (which caused
confusion to me):

\begin{itemize}
  \item $2 = 0 + 2$
  \item $2 = 2 + 0$
\end{itemize}

When I read the proof step ``Substracting these two equations'', I thought:
``What if I change the order of the elements and then substract the elements in
the changed order? This is addition, so I can change the order of them, right?
Think about the example of $2 = 0 + 2 = 2 + 0$. If I do $0 - 2$ and $2 - 0$, I
will get $(-2) + (2)$ which is still $0$, but I can't prove that $0$ and $2$
are euqal.''

This surely looks rediculous, but it took me a little while to realize my
mistake: The example $2 = 0 + 2 = 2 + 0$ is not appropriate. In this example,
the additive operands $0$ and $2$ both belong to the same set $\mathbf{N}$.
Therefore, they can be added and subtracted. However, in the case of $u_1 +
\ldots + u_m$ and $v_1 + \ldots + v_m$, only $u_1$ and $v_1$ belong to the same
set $U_1$. $u_1$ and $v_2$ belong to different sets. As a result, although we
can change the order of addition:

\begin{itemize}
  \item $v = u_1 + \ldots + u_m$, and
  \item $v = v_m + \ldots + v_1$
\end{itemize}

we can't subtract $v_m$ from $u_1$ because they belong to different sets. In
other words, to make the subtraction work, we have to subtract $v_1$ from $u_1$,
$\ldots$, $v_m$ from $u_m$. This is why $2 - 2$ can result in at least two
forms (there can be more): $(0 + 2) - (2 + 0) = (-2 + 2)$ and $(0 + 2) -
(0 + 2) = (0 + 0)$. But $v - v$ will surely result in exactly one form that is
mentioned in the textbook:

\[
  0 = (u_1 - v_1) + \ldots + (u_m - v_m)
\]

% ------------------------------
\section{1.45 Direct sum of \textbf{\textit{two}} subspaces}
% ------------------------------

Suppose $U$ and $W$ are subspaces of $V$. Then $U + W$ is a direct sum
\textbf{if and only if} $U \cap W = {0}$.

\textbf{NOTE}: This is only applicable to \textbf{\textit{exactly two}}
subspaces, as the last paragraph in the textbook says:

\begin{displayquote}
  The result above deals only with the case of two subspaces. When asking about
  a possible direct sum with more than two subspaces, it is \textbf{not enough}
  to test that each pair of the subspaces intersect only at 0. To see this,
  consider Example 1.43. In that nonexample of a direct sum, we have $U_1 \cap
  U_2 = U_1 \cap U_3 = U_2 \cap U_3 = {0}.$
\end{displayquote}

% ------------------------------
\section{Review Question Answers}
% ------------------------------

\begin{enumerate}
  \item See definition 1.19 in the textbook.
  \item $\mathbf{F}^n$ is just one concrete vector space, while $V$ can
    represent any vector space.
  \item The relationship is not defined. The vector space definition only
    defines commutativity for the addition on $u$ and $v$, but not the
    communtativity on the multiplication.
  \item The relationship is not defined. The vector space definition only
    defines associativity on addition and scalar multiplication, but not on
    the general multiplication of vectors inside the vector space.
  \item Because the definition of vector space depends on scalar multiplication
    which then depends on the scalar set $\mathbf{F}$. The elements in
    $\mathbf{F}$ participate the scalar multiplication which is part of vector
    space definition.
  \item See definition 1.23 in the textbook.
  \item $S$ stands for the set of the variable (i.e., the input parameter). An
    element in $\mathbf{F}^S$ is a function that maps from $S$ to $\mathbf{F}$.
  \item See the section \ref{F^S understanding}, item \ref{mapping rules}.
    \item Because the scalar multiplication in section 1.17 is only defined for
    $\mathbf{F}^n$, but the property ``1.29 The number 0 times a vector'' is a
    general property in any vector space $V$ which does not have to be
    $\mathbf{F}^n$. When $V$ is not $\mathbf{F}^n$, the scalar multiplication
    in section 1.17 may not make sense at all.
  \item See definition 1.32 in the textbook.
  \item See section 1.34 in the textbook.
  \item The three conditions for the subset $U$ are all about the elements and
    arithmetic operations on it. The multiplicative identity $1$ belongs to the
    set $\mathbf{F}$ (which is implied in the textbook due to notation 1.28),
    so the multiplicative identity has nothing to do with $U$, so it is not
    mentioned in the three conditions.
  \item See section 1.36 in the textbook.
  \item See section 1.40 in the textbook.
  \item A direct sum denotes a special case of sum of subsets. In regular sums
    of subsets, an element in $U_1 + \ldots + U_m$ may have one or more
    possible ways to add the elements from $U_1, \ldots, U_m$. But in direct
    sums, an element in $U_1 + \ldots + U_m$ has exactly one way to add the
    elements from $U_1, \ldots, U_m$.
\end{enumerate}

% ------------------------------
\section{Progress tracker}
% ------------------------------

As of 2023-04-25, I'm on page 20: \textit{1.39 Sum of subspaces is the smallest
containing subspace}. In the next study session, I need to prove it because the
textbook doesn't provide the full proof.

\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\begin{itemize}
  \item $[1]$ \href{https://linear.axler.net/}{Sheldon Axler: \it{Linear Algebra Done Right}}
  \item $[2]$ \href{https://bookstore.ams.org/view?ProductCode=CHEL/79}{Edmund Landau: \it{Foundations of Analysis}}
\end{itemize}

\end{document}
