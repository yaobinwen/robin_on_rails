% Regarding `oneside` (https://stackoverflow.com/a/8371473/630364):
%
% `oneside` removes the blank pages between chapters.
% "Note that this method make the margins of all the pages the same. In
% `twoside`, the margins are different for the odd and the even pages".
\documentclass[12pt, letterpaper, oneside]{book}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{float}
\usepackage{hyperref}
\usepackage[letterpaper, textwidth=7.5in, textheight=8in]{geometry}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
}
\usepackage{parskip}

\title{Notes on \textit{Linear Algebra Done Right}}
\author{Yaobin Wen}
\date{January 2023}

\begin{document}

\maketitle
\tableofcontents

\chapter*{Overview}
\addcontentsline{toc}{chapter}{Overview}

This document contains my study notes of the textbook \textit{Linear Algebra
Done Right}. I use it for a few purposes:

\begin{enumerate}
  \item As a reference to quickly refresh my memory on the subjects.
  \item Keep the notes to help me understand the text that is not obvious for
    me to comprehend.
\end{enumerate}

% =============================================================================
%
% Chapter: Review of Real Numbers
%
% =============================================================================

\chapter*{Review of Real Numbers}
\addcontentsline{toc}{chapter}{Review of Real Numbers}

(TODO)

% =============================================================================
%
% Chapter 1: Vector Spaces
%
% =============================================================================

\chapter{Vector Spaces}

% =============================================================================
\section{Review Questions}
% =============================================================================

(See the end of this chapter for the review question answers.)

\begin{enumerate}
  \item What is a vector space?
  \item What is the relationship between $\mathbf{F^n}$ and the vector space
    $V$?
  \item In a vector space $V$, for $u, v \in V$, what is the relationship
    between $u \cdot v$ and $v \cdot u$?
  \item In a vector space $V$, for $u, v, w \in V$, what is the relationship
  between $(u \cdot v) \cdot w$ and $u \cdot (v \cdot w)$?
  \item When talking about the vector space $V$ as the textbook defines it, why
    is it important to specify that it depends on the set $F$? What role does
    $\mathbf{F}$ play in the definition of $V$?
  \item What is $\mathbf{F}^S$?
  \item In the definition of $\mathbf{F}^S$, what does $S$ stand for? What is
    an element in $\mathbf{F}^S$?
  \item Does the definition of $\mathbf{F}^S$ imply that the function
    $0: S \rightarrow \mathbf{F}$ is an element in $\mathbf{F}^S$? Why or why
    not?
  \item In section ``1.29 The number 0 times a vector'', the textbook proves
    the property $0v = 0$ for every $v \in V$ without using the scalar
    multiplication in $\mathbf{F}^n$ in section 1.17. Why not use that? Should
    it have been used, the proof would have been much more simple, wouldn't it?
  \item What is a subspace?
  \item What are the conditions for the subset $U$ to satisfy in order to make
    it a subspace of $V$?
  \item Among all the conditions that $U$ must satisfy, what about the
    multiplicative identity $1$?
  \item What is the sum of subsets?
  \item What is a direct sum?
  \item How is direct sum different from sum of subsets?
\end{enumerate}

% =============================================================================
\section{Overview}
% =============================================================================

Overall, this chapter is all about the following key concepts:

\begin{itemize}
  \item $\mathbf{F}$: The unification of $\mathbf{R}$ and $\mathbf{C}$.
  \item \textbf{\textit{list}}.
  \item $\mathbf{F}^n$, which introduces \textbf{\textit{vector}} (on
    $\mathbf{F}^n$).
  \item A \textbf{\textit{vector space}} can be based on any set, not
    particularly on $\mathbf{F}^n$, but $\mathbf{F}^n$ and $\mathbf{F^S}$ are
    two important vector spaces.
  \item \textbf{\textit{Subspace}}
  \item \textbf{\textit{Direct sum}}
\end{itemize}

One needs to clearly understand not only these concepts but, more importantly,
also the relations among these concepts, especially which one is built on top
of which others.

% =============================================================================
\section{Arithmetics and properties (on numbers)}
% =============================================================================

% ******************************
\subsection{Abstract arithmetic operations}
% ******************************

To some extent, we don't have to define arithmetics on top of numbers. If we
want, we can define arithmetics on any set of objects, such as ``$\{ \clubsuit,
\diamondsuit, \heartsuit, \spadesuit \}$''. Later when we examine the concept
of vector spaces, we will see that a vector space is not necessarily based on a
set of numbers.

However, numeric sets still play an important role, and because we are quite
familiar with arithmetics that are defined on numeric sets, it's often helpful
(and more tangible) to use arithmetics on numeric sets to comprehend them and
think about their properties.

In this section, I will use $S$ to denote the set of objects (not necessarily
numbers) to discuss the arithmetics and properties. I will use the real numbers
$R$ to provide examples.

% ******************************
\subsection{Addition and multiplication}
% ******************************

Usually, we care about two arithmetic operations:

\begin{itemize}
  \item \textbf{Addition}: $\forall a, b \in S$, define $a + b$.
  \item \textbf{Multiplication}: $\forall a, b \in S$, define $a \cdot b$.
\end{itemize}

Because it's all up to us to define the rules of arithmetic operations, we can
define them in any ways that please us. For example, if $S$ is $\{ \clubsuit,
\diamondsuit, \heartsuit, \spadesuit \}$, we can define the following
arithmetic operations:

\begin{itemize}
  \item Additions:
    \begin{enumerate}
      \item $\clubsuit + \clubsuit = \clubsuit$
      \item $\clubsuit + \diamondsuit = \heartsuit$
      \item etc.
    \end{enumerate}
  \item Multiplications:
    \begin{enumerate}
      \item $\spadesuit \cdot \spadesuit = \clubsuit$
      \item $\spadesuit \cdot \heartsuit = \diamondsuit$
      \item etc.
    \end{enumerate}
\end{itemize}

However, such definitions can be pointless and have no value is real world.
Oftentimes, people prefer the definition of arithmetic operations that have
certain properties that can help them solve real world problems.

(Disclaimer: The following examples are derived from the replies by ChatGPT. I
did some quick verification and thought the replies were reasonable so I used
them. I myself am not at all a historian of mathematics so the examples above
could still be wrong.)

For example, in the ancient time, people may run into situations in which they
needed to combine or accumulate different quantities of objects. They gradually
realized that the combination or accumulation process of different objects can
be viewed as an abstract process of "adding things together", hence \textit{
addition}. The definition of addition had practical life meaning that helped
the ancient people finish their daily management work.

Similarly, the ancient people may encounter situations in which they needed to
calculate quantities that were repeated, such as multiple rows of objects.
ChatGPT gave me the following examples:

\begin{displayquote}
  For instance, ancient agricultural societies needed to determine the total
  amount of crops produced based on the number of fields or the number of rows
  and columns in each field. Merchants had to calculate the total value of
  goods by multiplying the quantity of items by their respective prices.
  Architects and builders needed to measure and calculate the areas of
  structures or lands, which involved scaling and multiplying dimensions.

  As people encountered these practical situations, they likely developed
  intuitive methods for combining or scaling quantities, leading to the
  discovery of multiplication as a formal mathematical operation.
\end{displayquote}

The conclusion is: Although we can define addition and multiplication in
whatever way we want, we usually prefer those that have some practical
foundations. (Well, I believe mathematics was initially derived from people's
everyday life, but later it became more like a "game of logic" and many
concepts were developed that showed no practical value at the time they were
invented, and only to find values years later.)

% ******************************
\subsection{Arithmetic properties}
% ******************************

If defined appropriately, the arithmetic operations can display the following
properties:

\begin{itemize}
  \item \textbf{Commutativity (on addition)}:
    \[ a + b = b + a, \forall a, b \in S \]
    For example: In $R$, $1 + 2 = 2 + 1$
  \item \textbf{Commutativity (on multiplication)}:
    \[ a \cdot b = b \cdot a, \forall a, b \in S \]
    For example: In $R$, $1 \cdot 2 = 2 \cdot 1$
  \item \textbf{Associativity (on addition)}:
    \[ (a + b) + c = a + (b + c), \forall a, b, c \in S \]
    For example: In $R$, $(1 + 2) + 3 = 1 + (2 + 3)$
  \item \textbf{Associativity (on multiplication)}:
    \[ (a \cdot b) \cdot c = a \cdot (b \cdot c), \forall a, b, c \in S \]
    For example: In $R$, $(1 \cdot 2) \cdot 3 = 1 \cdot (2 \cdot 3)$
  \item \textbf{Identity (on addition)}:
    \[ \exists z \in S, a + z = a, \forall a \in S \]
    For example: In $R$, $0$ is the additive identity.
  \item \textbf{Identity (on multiplication)}:
    \[ \exists o \in S, a \cdot o = a, \forall a \in S \]
    For example: In $R$, $1$ is the multiplicative identity.
  \item \textbf{Inverse (on addition)}:
    \begin{itemize}
      \item For every $a \in S$, there exists a unique $b \in S$ such that
        $a + b = z$, where $z$ is the additive identity in $S$.
      \item For example: In $R$, $-1$ is the additive inverse of $1$ because
        $1 + (-1) = 0$ (which is the additive identity).
      \item We denote the additive inverse as ``$-a$''.
    \end{itemize}
  \item \textbf{Inverse (on multiplication)}:
    \begin{itemize}
      \item For every $a \in S$ (and $a \neq z$), there exists a unique $b \in
        S$ such that $a \cdot b = o$, where $o$ is the multiplicative identity
        in $S$.
      \item For example: In $R$, $\frac{1}{2}$ is the multiplicative inverse of
        $2$ because $2 \cdot \frac{1}{2} = 1$ (which is the multiplicative
        identity).
      \item We denote the multiplicative inverse as ``$\frac{1}{a}$''.
    \end{itemize}
  \item \textbf{Distribution}:
    \[ a \cdot (b + c) = a \cdot b + a \cdot c, \forall a, b, c \in S \]
    For example: In $R$, $2 \cdot (1 + 3) = 2 \cdot 1 + 2 \cdot 3$.
\end{itemize}

% ******************************
\subsection{The two identities and two inverses}
% ******************************

When defining additions and multiplications, people notice that the set $S$ may
have two elements that play a special role in the arithmetics:

\begin{itemize}
  \item \textbf{Additive identity}: We denote it as $z$ (i.e., ``zero'').
  \item \textbf{Multiplicative identity}: We denote it as $o$ (i.e., ``one'').
\end{itemize}

The \textbf{additive inverse} and \textbf{multiplicative inverse} are defined
based upon them:

\begin{itemize}
  \item \textbf{Additive inverse}: An element $b \in S$ that "pulls" another
    element $a \in S$ back to the additive identity $z$ is called the additive
    inverse of $a$.
  \item \textbf{Multiplicative inverse}: An element $b \in S$ that "flattens"
    another element $a \in S$ to the multiplicative identity $o$ is called the
    multiplicative inverse.
\end{itemize}

These two inverses play an important role in defining the other two arithmetic
operations: subtraction and division.

% ******************************
\subsection{subtraction and division}
% ******************************

\textbf{\textit{Subtraction}} of $a, b \in S$ is defined as the addition of $a$
and the additive inverse of $b$:
\[ a - b = a + (-b)\]

\textbf{\textit{Division}} of $a, b \in S$ is defined as the multiplication of
$a$ and the multiplicative inverse of $b$:
\[ \frac{a}{b} = a \cdot \frac{1}{b} \]

The arithmetic operations \textit{subtraction} and \textit{division} are
defined around the additive identity and multiplicative identity.

For $\forall a \in S$, if there exists a unique element $b \in S$ such that
$a + b = z$ where $z$ is the additive identity on $S$, we call $b$ is the

% =============================================================================
\section{$\mathbf{F}$: Unification of $\mathbf{R}$ and $\mathbf{C}$}
% =============================================================================

This chapter starts with defining complex numbers (denoted by $\mathbf{C}$) and
the addition and multiplication on them (see Definition 1.1).

If you look at the definition of complex numbers, you will notice that complex
numbers completely include real numbers. In other words, real numbers are a
sub-set of complex numbers. So the complex numbers are already a unification of
both real numbers and the non-real complex numbers. \textbf{Then why don't we
just use $\mathbf{C}$ to represent both? Why do we want to use $\mathbf{F}$?}

It turns out to be that we want to unify not only the numbers themselves, but
also the arithmetic operations on them. Abstract algebra defines the concept of
"fields". Generally speaking, a \textbf{\textit{field}} is a mathematical
structure that consists of a set of elements along with two binary operations
(\textit{addition} and \textit{multiplication}) that satisfy certain properties.
It's not the topic of this book, but if there is a chance, we will learn that
both $\mathbf{R}$ and $\mathbf{C}$, along with the addition and multiplication
defined on them, are examples of fields. Therefore, we want to unify
$\mathbf{R}$ and $\mathbf{C}$ from the perspective of fields. That's why the
textbook also wants to discuss the arithmetics.

To do that, we need to show that addition and multiplication on complex numbers
show the same properties as the addition and multiplication on real numbers.

% ******************************
\subsection{1.1 Definition: Complex numbers}
% ******************************

We {define} a \textbf{\emph{complex number}} as follows (see
``1.1 Definition''):

\begin{itemize}
  \item A complex number is an ordered pair $(a, b)$, where $a, b \in
    \mathbf{R}$, but we will write this as $a + bi$.
    \begin{itemize}
      \item When $a, b \in \mathbf{R}$ and $b = 0$, we have all the real
        numbers $\mathbf{R}$. Thus $\mathbf{R} \subset \mathbf{C}$.
    \end{itemize}
  \item The set of all complex numbers is denoted by $\mathbf{C}$:
    \[ \mathbf{C} = \{a + bi: a, b \in \mathbf{R}\} \]
  \item \textbf{Addition}: Let $\alpha = (a + bi)$, $\beta = (c + di)$, where
    $a, b, c, d \in \mathbf{R}$, we have:
    \[
      \alpha + \beta = (a + bi) + (c + di) = (a + c) + (b + d)i \in \mathbf{C}
    \]
  \item \textbf{Multiplication}: Let $\alpha = (a + bi)$, $\beta = (c + di)$,
    where $a, b, c, d \in \mathbf{R}$, we have:
    \[
      \alpha\beta = (a + bi)(c + di) = (ac - bd) + (ad + bc)i \in \mathbf{C}
    \]
\end{itemize}

% ******************************
\subsection{1.3 Properties of complex arithmetic}
% ******************************

The following properties can be proved according to the definition of complex
numbers and the addition and multiplication:

\begin{itemize}
  \item \textbf{commutativity}:
    \[
      \alpha + \beta = \beta + \alpha \quad and \quad \alpha\beta\ =
      \beta\alpha \quad for \; all \; \alpha, \beta \in \mathbf{C}
    \]
  \item \textbf{associativity}:
    \[
      (\alpha + \beta) + \lambda = \alpha + (\beta + \lambda) \quad and \quad
      (\alpha\beta)\lambda = \alpha(\beta\lambda) \quad for \; all \; \alpha,
      \beta, \lambda \in \mathbf{C}
    \]
  \item \textbf{identities}:
    \[
      \lambda + 0 = \lambda \ and \ \lambda1 = \lambda \ for \ all \ \lambda
      \in \mathbf{C}
    \]
    \begin{itemize}
      \item Note that here $0, 1 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{additive inverse}:
    \[
      For \ every \ \alpha \in \mathbf{C}, there \ exists \ a \ unique \ \beta
      \in \mathbf{C} \ such \ that \ \alpha + \beta = 0
    \]
    \begin{itemize}
      \item Note that here $0 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{multiplicative inverse}:
    \[
      For \ every \ \alpha \in \mathbf{C} \ with \ \alpha \neq 0, there \
      exists \ a \ unique \ \beta \in \mathbf{C} \newline such \ that \
      \alpha\beta = 1
    \]
    \begin{itemize}
      \item Note that here $1 \in \mathbf{R}$
    \end{itemize}
  \item \textbf{distributive property}:
    \[
      \lambda(\alpha + \beta) = \lambda\alpha + \lambda\beta \ for \ all \
      \lambda, \alpha, \beta \in \mathbf{C}
    \]
\end{itemize}

NOTE(ywen): Complex numbers are defined based upon real numbers. In order to
prove the properties of complex arithmetic, we need to use the properties of
real arithmetic (i.e., commutativity, associativity, etc. for real numbers).
Example 1.4 shows this method.

% ******************************
\subsection{1.5 Definition: $-\alpha$, $1/\alpha$ ($\alpha \in \mathbf{C}$)}
% ******************************

Let $\alpha, \beta \in \mathbf{C}$:

\begin{itemize}
  \item ``$-\alpha$'' is the additive inverse of $\alpha$ such that
    \[ \alpha + (-\alpha) = 0 \]
    \begin{itemize}
      \item Note that here $0 \in \mathbf{R}$
    \end{itemize}
  \item For $\alpha \neq 0$, define $1/\alpha$ as the \textbf{multiplicative
    inverse} of $\alpha$ such that
    \[ \alpha(1/\alpha) = 1 \]
    \begin{itemize}
      \item Note that here $1 \in \mathbf{R}$
    \end{itemize}
\end{itemize}

% ******************************
\subsection{1.5 Definition: subtraction, division (on $\mathbf{C}$)}
% ******************************

Let $\alpha, \beta \in \mathbf{C}$:

\begin{itemize}
  \item \textbf{\textit{Subtraction}} on $\mathbf{C}$ is defined by
    \[ \beta - \alpha = \beta + (-\alpha) \]
  \item \textbf{\textit{Division}} on $\mathbf{C}$ is defined by
    \[ \beta/\alpha = \beta(1/\alpha) \]
    \begin{itemize}
      \item Note that here $1 \in \mathbf{R}$
    \end{itemize}
\end{itemize}

% ******************************
\subsection{1.6.1 Definition: $\alpha^m$ ($\alpha \in \mathbf{C}$)}
% ******************************

(The textbook defines $\alpha ^m$ on top of $\mathbf{F}$. But if a definition
is applicable to $\mathbf{F}$, it is also applicable to $\mathbf{C}$.)

For $\alpha \in \mathbf{C}$ and $m \in \mathbf{N}$, we define:

\[ \alpha^m = \underbrace{\alpha\cdot\cdot\cdot\alpha}_{m \ times} \]

Thus for all $\alpha, \beta \in \mathbf{C}$, $m, n \in \mathbf{N}$:

\begin{itemize}
  \item $(\alpha^m)^n = \alpha^{mn}$
  \item $(\alpha\beta)^m = \alpha^m\beta^m$
\end{itemize}

% ******************************
\subsection{1.6 Notation F}
% ******************************

Through the previous sections, we can see that addition and multiplication that
are defined on $\mathbf{R}$ and $\mathbf{C}$ display the same properties. Now
we can unify them: $\mathbf{F}$ stands for either $\mathbf{R}$ or $\mathbf{C}$.

(The symbol ``$\mathbf{F}$'' is usually used to denote a field.)

% =============================================================================
\section{\textbf{\textit{list}}}
% =============================================================================

The concept of \textit{list} is not built upon $\mathbf{F}$. It is defined
separately, but later we will use \textit{list} and $\mathbf{F}$ to define
$\mathbf{F}^n$.

% ******************************
\subsection{1.8 Definition: \textit{list}, \textit{length}}
% ******************************

For $n \in \mathbf{N}$, a \textbf{\textit{list}} of \textbf{\textit{length n}}
is an \textbf{ordered} collection of \textit{n} \textbf{elements}. It's denoted
as follows: \[ (x_1, x_2, ..., x_n) \]

Note that:
\begin{itemize}
  \item A list has a \textbf{finite} length.
  \item An element can be anything: a number, another list, a text string, etc.
  \item A list of length 0 is denoted as ``$()$''.
  \item A list of length n is also called an \textit{n-\textbf{tuple}}.
\end{itemize}

Two lists $(x_1, x_2, ..., x_n)$ and $(y_1, y_2, ..., y_m)$ are equal if and
only if:
\begin{itemize}
  \item $n = m$
  \item $x_i = y_i$ for $i = 1, 2, ..., n$
\end{itemize}

% ******************************
\subsection{Lists and Sets}
% ******************************

Table compares lists and sets:
\begin{table}[H]
\centering
\begin{tabular}{||c c c ||}
 \hline
   & Lists & Sets \\ [0.5ex]
 \hline
 \hline
 Length & Finite & Finite or infinite \\
 Order & Matters & Doesn't matter \\
 Repetition & Allows & Doesn't allow \\ [1ex]
 \hline
\end{tabular}
\caption{Compare lists and sets}
\label{table:lists_sets_comp}
\end{table}

% =============================================================================
\section{$\mathbf{F}^n$}
% =============================================================================

Based on the concept of $\mathbf{F}$ and \textit{list}, we can define
$\mathbf{F^n}$.

% ******************************
\subsection{1.10 Definition: $\mathbf{F^n}$ and $\mathbf{0}$}
% ******************************

We define $\mathbf{F^n}$ as the set of all lists of length $n$ of elements of
$\mathbf{F}$:

\[
  \mathbf{F^n} = \{(x_1, ..., x_n): x_j \in \mathbf{F} \ for \ j = 1, ..., n\}
\]

We say $x_j$ is the $j^{th}$ \textbf{\textit{coordinate}} of $(x_1, ..., x_n)$.

Let $\mathbf{0}$ denote the list of length $n$ whose coordinates are all $0$:

\[
  \mathbf{0} = (0, 0, ..., 0)
\]

% ******************************
\subsection{1.11 Note: $\mathbf{C^1}$ can be thought of as a plane}
% ******************************

On page 6, below Example 1.11, the textbook says: ``Similarly, $\mathbf{C}^1$
can be thought of as a plane.''

It is so because $\mathbf{C}^1$ is the set of complex numbers, each of which
consists of two parts: the real part and the imaginary part. Both parts are real
numbers and they can form a 2D plane, hence ``as a plane''.

% ******************************
\subsection{1.13 Definition: \textit{addition} in $\mathbf{F}^n$}
% ******************************

Given

\[
  \mathbf{F^n} = \{(x_1, ..., x_n): x_j \in \mathbf{F} \ for \ j = 1, ..., n\}
\]

and two lists $x, y \in \mathbf{F}^n$, and

\[
  x = (x_1, x_2, ..., x_n)
\]
\[
  y = (y_1, y_2, ..., y_n)
\]

we have

\begin{equation*}
\begin{split}
  x + y &= (x_1, x_2, ..., x_n) + (y_1, y_2, ..., y_n) \\
        &= (x_1 + y_1, x_2 + y_2, ..., x_n + y_n)
\end{split}
\end{equation*}

The addition has the property of \textbf{\textit{commutativity}}: If $x, y \in
\mathbf{F}^n$, then $x + y = y + x$.

The \textbf{\textit{additive inverse}} of $x$, denoted as $-x$, is the vector
$-x \in \mathbf{F}^n$ such that

\[
  x + (-x) = 0
\]

In other words, if $x = (x_1, ..., x_n)$, then $-x = (-x_1, ..., -x_n)$.

% ******************************
\subsection{
  1.14 Definition:\textbf{\textit{scalar multiplication}} in $\mathbf{F}^n$
}
% ******************************

For $\lambda \in \mathbf{F}$ and $(x_1, ..., x_n) \in \mathbf{F}^n$, we define

\[
  \lambda(x_1, ..., x_n) = (\lambda x_1, ..., \lambda x_n)
\]

% =============================================================================
\section{Vectors (in $\mathbf{F}^n$)}
% =============================================================================

For $x \in \mathbf{F}^n$, we can view it as a point in the space $\mathbf{F}^n$,
but we can also view it as an ``arrow'' in the same space. When we view it as
an ``arrow'', we refer to it as a \textbf{\textit{vector}}.

% =============================================================================
\section{Vector space}
% =============================================================================

In general, \textbf{\textit{vector spaces}} are not defined on $\mathbf{F}^n$
particularly. In fact, a vector space can be a set of objects along with
addition and scalar multiplication that meet certain conditions. Therefore, in
a general sense, we \textit{should not} assume that a vector space is
$\mathbf{F}^n$. However, $\mathbf{F}^n$ is an important vector space that we
will discuss a lot in the future.

We will use the symbol $\mathbf{V}$ to denote the general set.

% ******************************
\subsection{1.18 Definition: \it{addition}, \it{scalar multiplication}}
% ******************************

In order to define vector spaces in a general sense (i.e., not particularly on
$\mathbf{F}^n$), we need to firstly define \textit{addition} and \textit{scalar
multiplication} in a general sense:

\begin{itemize}
  \item An \textbf{\textit{addition}} on a set $V$ is a function that assigns
    an element $u+v \in V$ to each pair of element $u, v \in V$.
  \item A \textbf{\textit{scalar multiplication}} on a set $V$ is a function
    that assigns an element $\lambda v \in V$ to each $\lambda \in \mathbf{F}$
    and each $v \in V$.
\end{itemize}

% ******************************
\subsection{1.19 Definition: \textbf{\textit{vector space}}}
% ******************************

A \textbf{\textit{vector space}} is a set $V$ along with an \textit{addition}
on $V$ and a \textit{scalar multiplication} on $V$ such that the following
properties hold:

\textbf{commutativity}: $u + v = v + u$ for all $u, v \in V$. \textbf{Note}
that it doesn't define the relationship between $uv$ and $uv$.

\textbf{associativity}: For all $u, v, w \in V$ and all $a, b \in \mathbf{F}$:
\begin{itemize}
  \item $(u + v) + w = u + (v + w)$
  \item $(ab)v = a(bv)$
\end{itemize}

\textbf{Note} that associativity doesn't define the relationship between
$(uv)w$ and $u(vw)$.

\textbf{additive identity}: For all $v \in V$, there exists an element $0 \in V$
such that $v + 0 = v$. \textbf{Note} this doesn't require uniqueness of $0$.

\textbf{additive inverse}: For all $v \in V$, there exists $w \in V$ such that
$v + w = 0$. \textbf{Note} this doesn't require uniqueness of $w$.

\textbf{multiplicative identity}: $1v = v$ for all $v \in V$. \textbf{Note}
  that $1 \in \mathbf{F}$.

\textbf{distributive properties}: For all $a, b \in \mathbf{F}$ and all $u, v
\in V$:
\begin{itemize}
  \item $a(u + v) = au + av$
  \item $(a + b)v = av + bv$
\end{itemize}

% ******************************
\subsection{
  What do commutativity and associativity mean for additions on vector spaces?
}
% ******************************

The \textbf{additions} a vector space $V$ has the properties of commutativity
and associativity. What that means essentially is, for $u, v, w \in V$, one can
calculate the the \textbf{addition} of $u, v, w$ in \textbf{any order} with
\textbf{any association}. In other words, all the following calculations are
equal:

\begin{itemize}
  \item $u + v + w$
  \item $u + w + v$
  \item $u + (v + w)$
  \item $v + u + w$
  \item $v + (u + w)$
  \item $w + v + u$
  \item etc.
\end{itemize}

It's easy to prove that. But to prove it, we need to explicitly mention a rule:
\[ u + v + w = (u + v) + w \]

Namely, $u + v$ is just a simpler form of writing $(u + v)$ because there is no
ambiguity in the order of calculation. With that said, we have:

\begin{equation*}
  \begin{split}
    u + v + w &= (u + v) + w \\
              &= u + (v + w) \\
              &= u + (w + v) \\
              &= (u + w) + v = u + w + v \\
              &= (w + u) + v \\
              &= w + (u + v) \\
              &= w + (v + u) \\
              &= (w + v) + u \\
              &= w + v + u \\
              &= \ldots 
  \end{split}
\end{equation*}

The trick is to use the \textbf{commutativity} and \textbf{associativity}
repeatedly.

% ******************************
\subsection{Properties 1.25 - 1.31}
% ******************************

\begin{itemize}
  \item 1.25: Unique additive identity: A vector space has a unique additive
    identity.
  \item 1.26 Unique additive inverse: Every element in a vector space has a
    unique additive inverse.
  \item 1.27 Notation $-v$, $w - v$: Let $v, w \in V$, then:
    \begin{itemize}
      \item[$\bullet$] $-v$ denotes the additive inverse of $v$.
      \item[$\bullet$] $w - v$ is defined to be $w + (-v)$.
    \end{itemize}
  \item 1.28 Notation $V$: For the rest of the book, $V$ denotes a vector space
    over $\mathbf{F}$.
  \item 1.29 The number 0 times a vector: $0v = 0$ for every $v \in V$.
  \item 1.30 A number times the vector 0: $a0 = 0$ for every $a \in \mathbf{F}$.
  \item 1.31 The number $-1$ times a vector: $(-1)v = -v$ for every $v \in V$.
\end{itemize}

% ******************************
\subsection{My understanding of vector space}
% ******************************

\begin{itemize}
  \item Essentially, ``addition'' of two vectors is to connect the tail of the
    first vector with the head of the second vector.
  \item Essentially, ``scalar multiplication'' of a vector is to shrink or
    stretch the vector.
  \item The definition of \textit{vector space} is saying: For an arbitrary set
    $V$, we can define arbitrary operations called \textit{addition} and
    \textit{scalar multiplication} on the elements (assuming ``scalar'' makes
    sense in the context). However, not all the addition and scalar
    multiplication can make $V$ meet the properties that are required by vector
    space definition. In other occasions, $V$ may not include all the elements
    that make the addition and scalar multiplication meet the properties that
    are required by the vector space definition. Only those sets $V$ and the
    corresponding addition and scalar multiplication that meet those properties
    are called \textit{vector spaces}.
\end{itemize}

% =============================================================================
\section{Vector space $F^S$}
% =============================================================================

In addition to $\mathbf{F}^n$, $F^S$ is another important vector space that we
will use in the future.

% ******************************
\subsection{1.23 Notation $F^S$}
% ******************************

\begin{itemize}
  \item If $S$ is a set, then $\mathbf{F}^S$ denotes the set of
    \textbf{functions} from $S$ to $\mathbf{F}$.
  \item For $f,g \in \mathbf{F}^S$, the \textbf{sum} $f + g \in \mathbf{F}^S$
    is the function defined by
      \[ (f + g)(x) = f(x) + g(x) \]
    for all $x \in S$.
  \item For $\lambda \in \mathbf{F}$ and $f \in \mathbf{F}^S$, the
    \textbf{product} $\lambda f \in \mathbf{F}^S$ is the function defined by
      \[ (\lambda f)(x) = \lambda f(x) \]
    for all $x \in S$.
\end{itemize}

% ******************************
\subsection{1.24 Example $\mathbf{F}^S$ is a vector space}
% ******************************

\begin{itemize}
  \item If $S$ is a \textbf{nonempty set}, then $\mathbf{F}^S$ (with the
   operations of addition and scalar multiplication as defined in section 1.23)
   is a vector space over $\mathbf{F}$.
  \item The additive identity of $\mathbf{F}^S$ is the function $0:
    S \rightarrow \mathbf{F}$ defined by
      \[ 0(x) = 0 \]
    for all $x \in S$.
  \item For $f \in \mathbf{F}^S$, the additive inverse of $f$ is the function
    $-f: S \rightarrow \mathbf{F}$ defined by
      \[ (-f)(x) = -f(x) \]
    for all $x \in S$.
\end{itemize}

% ******************************
\subsection{My understanding of $\mathbf{F}^S$} \label{F^S understanding}
% ******************************

\begin{enumerate}
  \item Note that the elements in $\mathbf{F}^S$ are \textbf{functions}.
    According to the definition of $\mathbf{F}^S$, the only condition these
    functions must satisfy is they map from $S$ to $\mathbf{F}$.
  \item It's also important to note that it doesn't matter how the mapping from
    $S$ to $\mathbf{F}$ is defined. For example, it doesn't say that every
    element in $S$ must be mapped to a different element on $\mathbf{F}$. In
    fact, it's possible that all the element in $S$ are just mapped to one
    element in $\mathbf{F}$. Again, the point is the mapping itself does not
    matter, as long as the mapping is from $S$ to $\mathbf{F}$.
  \item \label{mapping rules} Because the mapping rule doesn't matter, that
    means \textbf{all possible mappings} (i.e., all possible functions) from
    $S$ to $\mathbf{F}$ are included in $\mathbf{F}^S$. For example:
    \begin{enumerate}
      \item Function $f_1: x \rightarrow x$ for all $x \in S$
      \item Function $f_2: x \rightarrow x + 1.339$ for all $x \in S$
      \item Function $f_3: x \rightarrow 2x^3 + 9\pi$ for all $x \in S$
      \item Function $f_4: x \rightarrow 1$ for all $x \in S$
      \item Function $f_5:$
        \[
          x \rightarrow \begin{cases}
            1, & if x = 13.5 \\
            0,  & if x \neq 13.5 \\
          \end{cases}
        \]
        for all $x \in S$.
      \item Function $f_6: x \rightarrow 0$ for all $x \in S$
      \item etc1.32 Definition.
    \end{enumerate}
    Notice that $f_6$ is the \textbf{additive identity} in $\mathbf{F}$.
  \item The type of elements of $S$ doesn't matter either, according to the
    definition. For example, the elements in $S$ don't have to be numbers. They
    can be shapes like $\{ \clubsuit, \diamondsuit, \heartsuit, \spadesuit \}$.
  \item The textbook says $\mathbf{F}^n$ and $\mathbf{F}^{\infty}$ can be
    viewed as special cases of $\mathbf{F}^S$. This is correct but the textbook
    doesn't explain everything about them:
    \begin{itemize}
      \item In these two cases, $S$ is the set $\{ 1, 2, \ldots, n \}$ and
      $\{ 1, 2, \ldots, \}$, respectively.
      \item An element in $\mathbf{F}^n$, i.e., $\{ x_1, x_2, \ldots, x_n \}$,
        is written as a list but represents a \textbf{function}. Yes, we
        usually use the symbol $f$ to denote a function, but we don't have to.
        If we want to, we can choose any arbitrary symbols to denote functions,
        including using a list. In the case of $\mathbf{F}^n$, the list
        $\{ x_1, x_2, \ldots, x_n \}$ and $\{ y_1, y_2, \ldots, y_n \}$ can be
        used to denote two different functions, just in the same way that $f$
        and $g$ can denote different functions.
    \end{itemize}
\end{enumerate}

% =============================================================================
\section{\textbf{V}, \textbf{vector}, and \textbf{point}}
% =============================================================================

% ******************************
\subsection{1.20 Definition: \textbf{\textit{vector}}, \textbf{\textit{point}}}
% ******************************

Elements of a vector space are called \textbf{\textit{vector}} or
\textbf{\textit{point}}.

% ******************************
\subsection{1.28 Notation \textbf{\textit{V}}}
% ******************************

For the rest of the book, \textbf{\textit{V}} denotes a vector space over
$\mathbf{F}$.

% =============================================================================
\section{Subspaces}
% =============================================================================

% ******************************
\subsection{1.32 Definition \textit{subspace}}
% ******************************

A subset $U$ of $V$ is called a \textbf{\textit{subspace}} of $V$ if $U$ is
also a vector space (using the same addition and scalar multiplication as on
$V$).

% ******************************
\subsection{1.34 Conditions for a subspace}
% ******************************

A subset $U$ of $V$ is a subspace of $V$ \textbf{if and only if} $U$ satisfies
the following three conditions:

\begin{itemize}
  \item \textbf{additive identity}: $0 \in U$
  \item \textbf{closed under addition}: $u, w \in U$ implies $u + w \in U$.
  \item \textbf{closed under scalar multiplication}: $\lambda \in \mathbf{F}$
    and $u \in U$ implies $\lambda u \in U$.
\end{itemize}

% ******************************
\subsection{1.36 Definition \textbf{\textit{sum of subsets}}}
% ******************************

\textbf{NOTE}: This definition is about \textbf{\textit{subsets}}, not
subspaces.

Suppose $U_1, \ldots, U_m$ are \textbf{subsets} of $V$. The \textbf{sum} of
$U_1, \ldots, U_m$, denoted $U_1 + \ldots + U_m$, is the set of \textbf{all
possible} sums of elements of $U_1, \ldots, U_m$. More precisely,

\[
U_1 + \ldots + U_m = \{ u_1 + \ldots + u_m: u_1 \in U_1, \ldots, u_m \in U_m \}
\]

% ******************************
\subsection{1.39 Sum of subspaces is the smallest containing subspace}
% ******************************

Suppose $U_1, \ldots, U_m$ are subspaces of $V$. Then $U_1 + \ldots + U_m$ is
the \textit{smallest subspace} of $V$ containing $U_1, \ldots, U_m$.

Proof: The proof consists of a few parts.

Part 1: Prove that $U_1 + \ldots + U_m$ is a subspace of $V$.

Firstly, let's prove that the additive identity $0 \in U_1 + \ldots + U_m$.
Because $U_1, \ldots, U_m$ are subspaces of $V$, according to the conditions of
subspaces, we know that the additive identity $0$ of $V$ must be in all of
$U_1, \ldots, U_m$. In other words:

\begin{align*}
      0 & \in U_1 \\
      0 & \in U_2 \\
        & \ldots \\
  + \ 0 & \in U_m \\
  = \ 0 & \in U_1 + U_2 + \ldots + U_m
\end{align*}

So we can see that additive identity $0 \in U_1 + \ldots + U_m$.

Secondly, let's prove that addition is closed under $U_1 + \ldots + U_m$. Let's
use $W$ to denote $U_1 + \ldots + U_m$, and the two elements $w_1, w_2 \in W$
can be denoted as follows:

\[
  w_1 = u_{11} + \ldots + u_{m1}, u_{11} \in U_1, \ldots, u_{m1} \in U_m
\]

and

\[
  w_2 = u_{12} + \ldots + u_{m2}, u_{12} \in U_1, \ldots, u_{m2} \in U_m
\]

So $w_1 + w_2 = (u_{11} + \ldots + u_{m1}) + (u_{12} + \ldots + u_{m2})$.

Because $u_{11}, \ldots, u_{m1}, u_{12}, \ldots, u_{m2} \in V$, the addition of
these elements can be performed in any order with any association. Therefore:

\begin{align*}
  w_1 + w_2 & = (u_{11} + \ldots + u_{m1}) + (u_{12} + \ldots + u_{m2}) \\
            & = u_{11} + \ldots + u_{m1} + u_{12} + \ldots + u_{m2} \\
            & = (u_{11} + u_{12}) + \ldots + (u_{m1} + u_{m2})
\end{align*}

Because $u_{11}, u_{12} \in U_1$ and $U_1$ is a subspace, that means $u_{11} +
u_{12} \in U_1$. The same can be applied to $u_{i1}, u_{i2}$ so we know
$u_{i1} + u_{i2} \in U_i$ where $1 \leq i \leq m$.

Therefore, we can tell that $w_1 + w_2$ is the sum of $u_i \in U_i$ where
$1 \leq i \leq m$, so $w_1 + w_2 \in U_1 + \ldots + U_m$. So addition is closed
under $U_1 + \ldots + U_m$.

Thirdly, let's prove that scalar multiplication is closed under $U_1 + \ldots +
U_m$ using the denotation above. For $\lambda \in \mathbf{F}$, we have

\[
  \lambda w_1 = \lambda (u_{11} + \ldots + u_{m1})
\]

Because $u_{11}, \ldots, u_{m1} \in V$, the distributive property holds, so

\begin{align*}
  \lambda w_1 & = \lambda (u_{11} + \ldots + u_{m1}) \\
              & = \lambda u_{11} + \ldots + \lambda u_{m1}
\end{align*}

Because $u_{11} \in U_1$ and $U_1$ is a subspace, that means $\lambda u_{11}
\in U_1$ because of the closure on scalar multiplication. The same can be
applied to $\lambda u_{i1}$ so we know $\lambda u_{i1} \in U_i$ where $1 \leq i
\leq m$.

Therefore, we can tell that $\lambda w_1$ is the sum of $\lambda u_{i1}$ where
$1 \leq i \leq m$, so $\lambda w_1 \in U_1 + \ldots + U_m$. So scalar
multiplication is closed under $U_1 + \ldots + U_m$.

Part 2: Prove that $U_1 + \ldots + U_m$ is the \textit{smallest} subspace of
$V$ containing $U_1, \ldots, U_m$.

Firstly, the textbook proves that $U_1, \ldots, U_m$ are all contained in
$U_1 + \ldots + U_m$. The textbook says:

\begin{displayquote}
  to see this, consider sums $u_1 + \ldots + u_m$ where all except one of the
  $u$'s are 0.
\end{displayquote}

What the textbook means is: Consider the sums $u_1 + u_2 + \ldots + u_m$ where
$u_2, \ldots, u_m$ are all the additive identity $0$. As a result, $u_1$ plus a
$m-1$ number of $0$ still produce $u_1$. Because $u_1$ means any arbitrary
element in $U_1$, that means $u_1 + u_2 + \ldots + u_m$ is essentially any
arbitrary element in $U_1$, so all the possible selection of $u_1$ will result
in the entire set $U_1$, which means the set $U_1$ is contained in $U_1 +
\ldots + U_m$. Then we can apply the same process to $u_i$ where $2 \leq i \leq
m$ to prove that the set $U_i$ is contained in $U_1 + \ldots + U_m$ where $2
\leq i \leq m$. Therefore, we can prove that $U_1, \ldots, U_m$ are all
contained in $U_1 + \ldots + U_m$.

Secondly, the textbook proves that every subspace of $V$ containing $U_1,
\ldots, U_m$ also contains $U_1 + \ldots + U_m$. The textbook says:

\begin{displayquote}
  ...because subspaces must contain all finite sums of their elements
\end{displayquote}

What the textbook means is: Now we are considering a subspace of $V$ that
must contain $U_1, \ldots, U_m$. Let's denote this subspace with $U$. Because
$U$ is a subspace, it must be closed under addition, which means $\forall u, w
\in U$, we have $u + w \in U$. Because $U_1$ and $U_2$ are contained in $U$,
$u_1 \in U_1 \Rightarrow u_1 \in U$, and $u_2 \in U_1 \Rightarrow u_2 \in U$,
therefore, $u_1 + u_2 \in U$. Let's denote $u_1 + u_2$ as $w_{u_1+u_2}$, so we
have $w_{u_1+u_2} \in U$. Now, because $U_3$ is contained in $U$, $\forall u_3
\in U_3 \Rightarrow u_3 \in U$. Therefore, $w_{u_1+u_2} + u_3 \in U \Rightarrow
(u_1 + u_2) + u_3 \in U \Rightarrow u_1 + u_2 + u_3 \in U$. We can apply the
same process for $u_i$ where $4 \leq i \leq m$. Once we get to $m$, we have
$u_1 + u_2 + \ldots + u_m \in U$. This means $\forall u_1 + u_2 + \ldots + u_m
\in U$, so we know $U_1 + \ldots + U_m \subseteq U$. In other words, every
subspace of $V$ containing $U_1, \ldots, U_m$ also contains $U_1 + \ldots +
U_m$. In other words, if a subset of $V$ does not contain $U_1 + \ldots + U_m$,
it can't be a subspace of $V$ containing $U_1, \ldots, U_m$. Therefore, any
subset $W \subset U_1 + \ldots + U_m$ is not such a subspace. The smallest
subset of $V$ that contains $U_1 + \ldots + U_m$ is $U_1 + \ldots + U_m$
itself. Because $U_1 + \ldots + U_m$ is also a subspace of $V$ containing $U_1,
\ldots, U_m$, we know $U_1 + \ldots + U_m$ is the \textit{smallest subspace} of
$V$ containing $U_1, \ldots, U_m$.

% =============================================================================
\section{Direct sum}
% =============================================================================

% ******************************
\subsection{1.40 Definition \textbf{\textit{direct sum}}}
% ******************************

Suppose $U_1, \ldots, U_m$ are subspaces of $V$:
\begin{itemize}
  \item The sum $U_1 + \ldots + U_m$ is called a \textbf{\textit{direct sum}}
    if each element of $U_1 + \ldots + U_m$ can be written in \textbf{only one
    way} as a sum $u_1 + \ldots + u_m$, where each $u_j$ is in $U_j$.
  \item If $U_1 + \ldots + U_m$ is a direct sum, then $U_1 \oplus \ldots \oplus
    U_m$ denotes $U_1 + \ldots + U_m$, with the $\oplus$ notation serving as an
    indication that this is a direct sum.
\end{itemize}

Example 1.41: Suppose $U$ is the subpsace of $\mathbf{F}^3$ of those vectors whose
last coordinate equals $0$, and $W$ is the subspace of $\mathbf{F}^3$ of those
vectors whose first two coordinates equal $0$:
\[
  U = {(x, y, 0) \in \mathbf{F}^3: x, y \in \mathbf{F}}
\]
and
\[
  W = {(0, 0, z) \in \mathbf{F}^3: x, y \in \mathbf{F}}
\]
Then $\mathbf{F}^3 = U \oplus W$.

Proof: Suppose any element $(x, y, z)$ in $U+W$ can be expressed in two ways:
\begin{enumerate}
  \item $(x, y, z) = (x_1, y_1, 0) + (0, 0, z_1) = (x_1 + 0, y_1 + 0, 0 + z_1)$
  \item $(x, y, z) = (x_2, y_2, 0) + (0, 0, z_2) = (x_2 + 0, y_2 + 0, 0 + z_2)$
\end{enumerate}

So we have:
\begin{enumerate}
  \item $x_1 + 0 = x = x_2 + 0 \Rightarrow x_1 = x_2$
  \item $y_1 + 0 = y = y_2 + 0 \Rightarrow y_1 = y_2$
  \item $z_1 + 0 = z = z_2 + 0 \Rightarrow z_1 = z_2$
\end{enumerate}

Therefore, $(x, y, z)$ has only one way to be expressed, so $U+W$ is a direct
sum and can be denoted as $U \oplus W$.

% ******************************
\subsection{Example 1.43: A question}
% ******************************

See the textbook for the example description and proof.

The point of this example is to show that $U_1 + U_2 + U_3$ is not a direct
sum. According to the definition of direct sum, a direct sum is defined on the
subspaces within a certain vector space $V$. However, the defintion does not
say that the direct sum of the subspaces must be equal to the entire vector
space $V$.

Therefore, I was confused by the proof of this example that's provided by the
textbook, because the paragraph after ``However'' seems to change the question
from proving that $U_1 + U_2 + U_3$ is not a direct sum to that
``$\mathbf{F}^3$ does not equal the direct sum of $U_1, U_2, U_3$.''

If we want to prove that $U_1 + U_2 + U_3$ is not a direct sum, we only need to
find one counter-example, which the proof already suggests: For any element
$(x, y, z)$ in $U_1 + U_2 + U_3$, we can find at least two expressions:

\begin{enumerate}
  \item $(x, y, z) = (x, y, 0) + (0, 0, z) + (0, 0, 0)$, and
  \item $(x, y, z) = (x, y + 1, 0) + (0, 0, z + 1) + (0, -1, -1)$
\end{enumerate}

This proves that there is no unique way to write the sum, so $U_1 + U_2 + U_3$
is not a direct sum.

% ******************************
\subsection{1.44 Condition for a direct sum}
% ******************************

Suppose $U_1, \ldots, U_m$ are subspaces of $V$. Then $U_1 + \ldots + U_m$ is a
direct sum \textbf{if and only if} the only way to write $0$ as a sum $u_1 +
\ldots + u_m$, where each $u_j$ is in $U_j$, is by taking each $u_j$ equal to 0.

A note about the proof: While studying the proof, when I saw the following
expressions:

\begin{itemize}
  \item $v = u_1 + \ldots + u_m$, and
  \item $v = v_1 + \ldots + v_m$
\end{itemize}

I started to think about the similar expressions in integers like (which caused
confusion to me):

\begin{itemize}
  \item $2 = 0 + 2$
  \item $2 = 2 + 0$
\end{itemize}

When I read the proof step ``Substracting these two equations'', I thought:
``What if I change the order of the elements and then substract the elements in
the changed order? This is addition, so I can change the order of them, right?
Think about the example of $2 = 0 + 2 = 2 + 0$. If I do $0 - 2$ and $2 - 0$, I
will get $(-2) + (2)$ which is still $0$, but I can't prove that $0$ and $2$
are euqal.''

This surely looks rediculous, but it took me a little while to realize my
mistake: The example $2 = 0 + 2 = 2 + 0$ is not appropriate. In this example,
the additive operands $0$ and $2$ both belong to the same set $\mathbf{N}$.
Therefore, they can be added and subtracted. However, in the case of $u_1 +
\ldots + u_m$ and $v_1 + \ldots + v_m$, only $u_1$ and $v_1$ belong to the same
set $U_1$. $u_1$ and $v_2$ belong to different sets. As a result, although we
can change the order of addition:

\begin{itemize}
  \item $v = u_1 + \ldots + u_m$, and
  \item $v = v_m + \ldots + v_1$
\end{itemize}

we can't subtract $v_m$ from $u_1$ because they belong to different sets. In
other words, to make the subtraction work, we have to subtract $v_1$ from $u_1$,
$\ldots$, $v_m$ from $u_m$. This is why $2 - 2$ can result in at least two
forms (there can be more): $(0 + 2) - (2 + 0) = (-2 + 2)$ and $(0 + 2) -
(0 + 2) = (0 + 0)$. But $v - v$ will surely result in exactly one form that is
mentioned in the textbook:

\[
  0 = (u_1 - v_1) + \ldots + (u_m - v_m)
\]

% ******************************
\subsection{1.45 Direct sum of \textbf{\textit{two}} subspaces}
% ******************************

Suppose $U$ and $W$ are subspaces of $V$. Then $U + W$ is a direct sum
\textbf{if and only if} $U \cap W = {0}$.

\textbf{NOTE}: This is only applicable to \textbf{\textit{exactly two}}
subspaces, as the last paragraph in the textbook says:

\begin{displayquote}
  The result above deals only with the case of two subspaces. When asking about
  a possible direct sum with more than two subspaces, it is \textbf{not enough}
  to test that each pair of the subspaces intersect only at 0. To see this,
  consider Example 1.43. In that nonexample of a direct sum, we have $U_1 \cap
  U_2 = U_1 \cap U_3 = U_2 \cap U_3 = {0}.$
\end{displayquote}

% =============================================================================
\section{Review Question Answers}
% =============================================================================

\begin{enumerate}
  \item See definition 1.19 in the textbook.
  \item $\mathbf{F}^n$ is just one concrete vector space, while $V$ can
    represent any vector space.
  \item The relationship is not defined. The vector space definition only
    defines commutativity for the addition on $u$ and $v$, but not the
    commutativity on the multiplication.
  \item The relationship is not defined. The vector space definition only
    defines associativity on addition and scalar multiplication, but not on
    the general multiplication of vectors inside the vector space.
  \item Because the definition of vector space depends on scalar multiplication
    which then depends on the scalar set $\mathbf{F}$. The elements in
    $\mathbf{F}$ participate the scalar multiplication which is part of vector
    space definition.
  \item See definition 1.23 in the textbook.
  \item $S$ stands for the set of the variable (i.e., the input parameter). An
    element in $\mathbf{F}^S$ is a function that maps from $S$ to $\mathbf{F}$.
  \item See the section \ref{F^S understanding}, item \ref{mapping rules}.
    \item Because the scalar multiplication in section 1.17 is only defined for
    $\mathbf{F}^n$, but the property ``1.29 The number 0 times a vector'' is a
    general property in any vector space $V$ which does not have to be
    $\mathbf{F}^n$. When $V$ is not $\mathbf{F}^n$, the scalar multiplication
    in section 1.17 may not make sense at all.
  \item See definition 1.32 in the textbook.
  \item See section 1.34 in the textbook.
  \item The three conditions for the subset $U$ are all about the elements and
    arithmetic operations on it. The multiplicative identity $1$ belongs to the
    set $\mathbf{F}$ (which is implied in the textbook due to notation 1.28),
    so the multiplicative identity has nothing to do with $U$, so it is not
    mentioned in the three conditions.
  \item See section 1.36 in the textbook.
  \item See section 1.40 in the textbook.
  \item A direct sum denotes a special case of sum of subsets. In regular sums
    of subsets, an element in $U_1 + \ldots + U_m$ may have one or more
    possible ways to add the elements from $U_1, \ldots, U_m$. But in direct
    sums, an element in $U_1 + \ldots + U_m$ has exactly one way to add the
    elements from $U_1, \ldots, U_m$.
\end{enumerate}

% =============================================================================
%
% Chapter 2: Finite-Dimensional Vector Spaces
%
% =============================================================================

\chapter{Finite-Dimensional Vector Spaces}

% =============================================================================
\section{Overview}
% =============================================================================

TODO

% =============================================================================
\section{Review Questions}
% =============================================================================

(See the end of this chapter for the review question answers.)

TODO

% =============================================================================
\section{2.2 Notation \textbf{\textit{list of vectors}}}
% =============================================================================

We will still write lists of numbers surrounded by parentheses; for example,
$(2, -7, 8) \in \mathbf{F}^3$.

We will usually write lists of vectors \textbf{without surrounding parentheses}.

For example, ``$(4, 1, 6), (9, 5, 7)$'' is a list of 2 vectors in
$\mathbf{R}^3$.

% =============================================================================
\section{2.3 Definition \textbf{\textit{linear combination}}}
% =============================================================================

A \textbf{\textit{linear combination}} of a list $v_1, \ldots, v_m$ of vectors
in $V$ is a vector of the form

\[
  a_1v_1 + \dots + a_mv_m
\]

where $a_1, \ldots, a_m \in \mathbf{F}$.

Examples:

\begin{itemize}
  \item $(17, -4, 2)$ is a linear combination of $(2, 1, -3)$ and $(1, -2, 4)$
    because
    \[
      (17, -4, 2) = 6 \cdot (2, 1, -3) + 5 \cdot (1, -2, 4)
    \]
  \item $(17, -4, 5)$ is not a linear combination of $(2, 1, -3)$ and
    $(1, -2, 4)$ because there do not exist numbers $a_1, a_2 \in \mathbf{F}$
    such that $(17, -4, 2) = a_1 \cdot (2, 1, -3) + a_2 \cdot (1, -2, 4)$.
\end{itemize}

% =============================================================================
\section{Definition \textbf{\textit{span}}}
% =============================================================================

The set of all linear combinations of a list of vectors $v_1, \ldots, v_m$ in
$V$ is called the \textbf{\textit{span}} of $v_1, \ldots, v_m$, denoted
$span(v_1, \ldots, v_m)$. In other words,

\[
  span(v_1, \ldots, v_m) = {a_1v_1 + \ldots + a_mv_m: a_1, \ldots, a_m \in
  \mathbf{F}}
\]

Examples:

\begin{itemize}
  \item $(17, -4, 2) \in span\bigl((2, 1, -3), (1, -2, 4)\bigr)$
  \item $(17, -4, 5)$ is not a linear combination of $(2, 1, -3)$ and
    $(1, -2, 4)$ because there do not exist numbers $a_1, a_2 \in \mathbf{F}$
    such that $(17, -4, 2) = a_1 \cdot (2, 1, -3) + a_2 \cdot (1, -2, 4)$.
\end{itemize}

% =============================================================================
\section{Definition \textbf{\textit{span of empty list}}}
% =============================================================================

The span of the empty list $()$ is defined to be ${0}$.

% =============================================================================
\section{Span is the smallest containing subspace}
% =============================================================================

The span of a list of vectors in $V$ is the smallest subspace of $V$ containing
all the vectors in the list.

The textbook has provided the proof so I won't repeat it here. I only want to
explain one step further.

In the proof, the textbook says:

\begin{displayquote}
  Furthermore, $span(v_1, \ldots, v_m)$ is closed under scalar multiplication,
  because
  \[
    \lambda(a_1v_1 + \dots + a_mv_m) = \lambda a_1v_1 + \dots + \lambda a_mv_m
  \]
\end{displayquote}

The proof is correct, but it doesn't clearly show the readers the detailed
reasoning process. The full process is as follows:

\begin{align}
  \lambda(a_1v_1 + \dots + a_mv_m)
    & = \lambda(a_1v_1) + \dots + \lambda (a_mv_m) \\
    & = (\lambda a_1)v_1 + \dots + (\lambda a_m)v_m
\end{align}

Remember $v_1, \ldots, v_m$ are all elements in $V$ which is a vector space,
so the arithmetic operations have the distributive property according to the
definition of vector space (see 1.19 vector space). Therefore $(2.1)$ holds.
$(2.2)$ holds because of the associativity on a vector space (see 1.19 vector
space).

Because $\lambda, a_1, \ldots, a_m \in \mathbf{F}$, their multiplications are
still in the set $\mathbf{F}$, so $(\lambda a_1)v_1 + \dots + (\lambda a_m)v_m$
is still in the span of $v_1, \ldots, v_m$. Therefore, we can reach the
conclusion that $span(v_1, \ldots, v_m)$ is closed under scalar multiplication.

% =============================================================================
\section{2.8 Definition \textbf{\textit{spans}} (as a verb)}
% =============================================================================

If $span(v_1, \ldots, v_m)$ equals $V$, we say that $v_1, \ldots, v_m$ \textbf{
\textit{spans}} $V$.

Example: The following vectors

\[
  (1, 0, \ldots, 0), (1, 0, \ldots, 0), (0, \ldots, 0, 1)
\]

spans $\mathbf{F}^n$.

% =============================================================================
\section{2.10 Definition \textbf{\textit{finite-dimensional vector space}}}
% =============================================================================

A vector space is called \textbf{\textit{finite-dimensional}} if some list of
vectors in it spans the space.

Pay close attention to the wording:

\begin{enumerate}
  \item The use of ``some list ...'' instead of ``some lists ...'' means:
  \begin{enumerate}
    \item It is just one list.
    \item As long as such a list exists, the definition holds.
  \end{enumerate}
  \item A \textit{list}, by definition, has finite length.
  \item This is a list of \textit{vectors}.
\end{enumerate}

% =============================================================================
\section{2.11 Definition \textbf{\textit{polynomial}}, $\mathcal{P}(\mathbf{F})$}
% =============================================================================

\begin{itemize}
  \item A function $p: \mathbf{F} \rightarrow \mathbf{F}$ is called a \textbf{
    \textit{polynomial}} with coefficients in $\mathbf{F}$ if there exist $a_0,
    \ldots, a_m \in \mathbf{F}$ such that
    \[
      p(z) = a_0 + a_{1}z + a_{2}z^2 + \dots + a_{m}z^m
    \]
    for all $z \in \mathbf{F}$.
  \item $\mathcal{P}(\mathbf{F})$ is the set of all polynomials with
    coefficients in $\mathbf{F}$.
\end{itemize}

% =============================================================================
\section{2.12 Definition \textbf{\textit{degree of a polynomial}}, deg $p$}
% =============================================================================

\begin{itemize}
  \item A polynomial $p \in \mathcal{P}(\mathbf{F})$ is said to have \textbf{
    \textit{degree}} $m$ if there exist scalars $a_0, a_1, \ldots, a_m \in
    \mathbf{F}$ with $a_m \neq 0$ such that
    \[
      p(z) = a_0 + a_{1}z + \dots + a_{m}z^m
    \]
    for all $z \in \mathbf{F}$. If $p$ has degree $m$, we write deg $p$ = $m$.
  \item The polynomial that is identically 0 is said to have degree $-\infty$.
\end{itemize}

We define the degree of the polynomial $0$ as $-\infty$ because we want to use
the convention that $-\infty < m$ in the definition of $\mathcal{P}_m(\mathbf{
F})$ in order to include the polynomial $0$ into it.

% =============================================================================
\section{2.13 Definition $\mathcal{P}_m(\mathbf{F})$}
% =============================================================================

For $m$ a nonnegative integer, $\mathcal{P}_m(\mathbf{F})$ denotes the set of
all polynomials with coefficients in $\mathbf{F}$ and degree at most $m$.

% =============================================================================
\section{2.15 Definition \textbf{\textit{infinite-dimensional vector space}}}
% =============================================================================

A vector space is called \textbf{\textit{infinite-dimensional}} if it is not
finite-dimensional.

Example: $\mathcal{P}(\mathbf{F})$ is infinite-dimensional.

% =============================================================================
\section{Review Question Answers}
% =============================================================================

(TODO)

\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\begin{itemize}
  \item $[1]$ \href{https://linear.axler.net/}{Sheldon Axler: \it{Linear Algebra Done Right}}
  \item $[2]$ \href{https://bookstore.ams.org/view?ProductCode=CHEL/79}{Edmund Landau: \it{Foundations of Analysis}}
\end{itemize}

\end{document}
